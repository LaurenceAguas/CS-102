"title","author","subject","abstract","date"
"advancing biomedical text mining with community challenges","hui zong, rongrong wu, jiaxue cha, erman wu, jiakun li, liang tao, zuofeng li, buzhou tang, bairong shen","artificial intelligence","the field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. however, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. to address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. these challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. in this article, we review the recent advances in community challenges specific to chinese biomedical text mining. firstly, we collect the information of these evaluation tasks, such as data sources and task types. secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation. then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective. finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.",2024-03-07
"measuring technological convergence in encryption technologies with proximity indices: a text mining and bibliometric analysis using openalex","alessandro tavazzi, dimitri percia david, julian jang-jaccard, alain mermoud","computers and society","identifying technological convergence among emerging technologies in cybersecurity is crucial for advancing science and fostering innovation. unlike previous studies focusing on the binary relationship between a paper and the concept it attributes to technology, our approach utilizes attribution scores to enhance the relationships between research papers, combining keywords, citation rates, and collaboration status with specific technological concepts. the proposed method integrates text mining and bibliometric analyses to formulate and predict technological proximity indices for encryption technologies using the ""openalex"" catalog. our case study findings highlight a significant convergence between blockchain and public-key cryptography, evidenced by the increasing proximity indices. these results offer valuable strategic insights for those contemplating investments in these domains.",2024-03-03
"text mining in education","r. ferreira-mello, m. andre, a. pinheiro, e. costa, c. romero","information retrieval","the explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. it produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. in this line, this work presents a systematic overview of the current status of the educational text mining field. our final goal is to answer three main research questions: which are the text mining techniques most used in educational environments? which are the most used educational resources? and which are the main applications or educational goals? finally, we outline the conclusions and the more interesting future trends.",2024-02-11
"is chatgpt the future of causal text mining? a comprehensive evaluation and analysis","takehiro takayanagi, masahiro suzuki, ryotaro kobayashi, hiroki sakaji, kiyoshi izumi","computation and language","causality is fundamental in human cognition and has drawn attention in diverse research fields. with growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. this study conducts comprehensive evaluations of chatgpt's causal text mining capabilities. firstly, we introduce a benchmark that extends beyond general english datasets, including domain-specific and non-english datasets. we also provide an evaluation framework to ensure fair comparisons between chatgpt and previous approaches. finally, our analysis outlines the limitations and future challenges in employing chatgpt for causal text mining. specifically, our analysis reveals that chatgpt serves as a good starting point for various datasets. however, when equipped with a sufficient amount of training data, previous models still surpass chatgpt's performance. additionally, chatgpt suffers from the tendency to falsely recognize non-causal sequences as causal sequences. these issues become even more pronounced with advanced versions of the model, such as gpt-4. in addition, we highlight the constraints of chatgpt in handling complex causality types, including both intra/inter-sentential and implicit causality. the model also faces challenges with effectively leveraging in-context learning and domain adaptation. we release our code to support further research and development in this field.",2024-02-22
"hunflair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools","mario s√§nger, samuele garda, xing david wang, leon weber-genzel, pia droop, benedikt fuchs, alan akbik, ulf leser","computation and language","with the exponential growth of the life science literature, biomedical text mining (btm) has become an essential technology for accelerating the extraction of insights from publications. identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in btm pipelines to enable information aggregation from different documents. however, tools for these two steps are rarely applied in the same context in which they were developed. instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. this raises the question of whether the reported performance of btm tools can be trusted for downstream applications. here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training. based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. hunflair2 showed the best performance on average, being closely followed by pubtator. our results indicate that users of btm tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make btm tools more robust.",2024-02-19
"construction of a syntactic analysis map for yi shui school through text mining and natural language processing research","hanqing zhao, yuehan li","computation and language","entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. most of the information of the yishui school of traditional chinese medicine (tcm) is stored in the form of unstructured classical chinese text. the key information extraction of tcm texts plays an important role in mining and studying the academic schools of tcm. in order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional chinese medicine texts, and uses the common weighting technology of tf-idf information retrieval and data mining to extract important key entity information in different ancient books. the dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of yishui school and the use of artificial intelligence methods to carry out the research of tcm academic schools.",2024-02-16
"multi-fidelity methods for optimization: a survey","ke li, fan li","machine learning","real-world black-box optimization often involves time-consuming or costly experiments and simulations. multi-fidelity optimization (mfo) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. this survey presents a systematic exploration of mfo, underpinned by a novel text mining framework based on a pre-trained language model. we delve deep into the foundational principles and methodologies of mfo, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. additionally, this survey highlights the diverse applications of mfo across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of mfo in tackling complex computational challenges. furthermore, we also envision several emerging challenges and prospects in the mfo landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level. we also address critical issues related to benchmarking and the advancement of open science within the mfo community. overall, this survey aims to catalyze further research and foster collaborations in mfo, setting the stage for future innovations and breakthroughs in the field.",2024-02-15
"a step towards the integration of machine learning and small area estimation","tomasz ≈ºƒÖd≈Ço, adam chwila","methodology","the use of machine-learning techniques has grown in numerous research areas. currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. however, the usage of these methods in survey sampling including small area estimation is still very limited. therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. we study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. what is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. the solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. the simulation studies are based on a real, longitudinal dataset, freely available from the polish local data bank, where the prediction problem of subpopulation characteristics in the last period, with ""borrowing strength"" from other subpopulations and time periods, is considered.",2024-02-12
"how to refactor this code? an exploratory study on developer-chatgpt refactoring conversations","eman abdullah alomar, anushkrishna venkatakrishnan, mohamed wiem mkaouer, christian d. newman, ali ouni","software engineering","large language models (llms), like chatgpt, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with chatgpt. in this paper, our goal is to explore conversations between developers and chatgpt related to refactoring to better understand how developers identify areas for improvement in code and how chatgpt addresses developers' needs. our approach relies on text mining refactoring-related conversations from 17,913 chatgpt prompts and responses, and investigating developers' explicit refactoring intention. our results reveal that (1) developer-chatgpt conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while chatgpt typically includes the refactoring intention; and (3) various learning settings when prompting chatgpt in the context of refactoring. we envision that our findings contribute to a broader understanding of the collaboration between developers and ai models, in the context of code refactoring, with implications for model improvement, tool development, and best practices in software engineering.",2024-02-08
"quantifying similarity: text-mining approaches to evaluate chatgpt and google bard content in relation to biomedical literature","jakub klimczak, ahmed abdeen hamed","computation and language","background: the emergence of generative ai tools, empowered by large language models (llms), has shown powerful capabilities in generating content. to date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. objectives using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. methods in this exploratory analysis, (1) we prompt-engineer chatgpt and google bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. results the experiments demonstrated that chatgpt outperformed google bard in cosine document similarity (38% to 34%), jaccard document similarity (23% to 19%), tf-idf bigram similarity (47% to 41%), and term network centrality (degree and closeness). we also found new links that emerged in chatgpt bigram networks that did not exist in literature bigram networks. conclusions: the obtained similarity results show that chatgpt outperformed google bard in document similarity, bigrams, and degree and closeness centrality. we also observed that chatgpt offers linkage to terms that are connected in the literature. such connections could inspire asking interesting questions and generate new hypotheses.",2024-01-19
"leveraging social media data to identify factors influencing public attitude towards accessibility, socioeconomic disparity and public transportation","khondhaker al momin, arif mohaimin sadri, md sami hasnine","computers and society","this study proposes a novel method to understand the factors affecting individuals' perception of transport accessibility, socioeconomic disparity, and public infrastructure. as opposed to the time consuming and expensive survey-based approach, this method can generate organic large-scale responses from social media and develop statistical models to understand individuals' perceptions of various transportation issues. this study retrieved and analyzed 36,098 tweets from new york city from march 19, 2020, to may 15, 2022. a state-of-the-art natural language processing algorithm is used for text mining and classification. a data fusion technique has been adopted to generate a series of socioeconomic traits that are used as explanatory variables in the model. the model results show that females and individuals of asian origin tend to discuss transportation accessibility more than their counterparts, with those experiencing high neighborhood traffic also being more vocal. however, disadvantaged individuals, including the unemployed and those living in low-income neighborhoods or in areas with high natural hazard risks, tend to communicate less about such issues. as for socioeconomic disparity, individuals of asian origin and those experiencing various types of air pollution are more likely to discuss these topics on twitter, often with a negative sentiment. however, unemployed, or disadvantaged individuals, as well as those living in areas with high natural hazard risks or expected losses, are less inclined to tweet about this subject. lack of internet accessibility could be a reason why many disadvantaged individuals do not tweet about transport accessibility and subsidized internet could be a possible solution.",2024-01-22
"a scalable and automated framework for tracking the likely adoption of emerging technologies","lowri williams, eirini anthi, pete burnap","computers and society","while new technologies are expected to revolutionise and become game-changers in improving the efficiencies and practises of our daily lives, it is also critical to investigate and understand the barriers and opportunities faced by their adopters. such findings can serve as an additional feature in the decision-making process when analysing the risks, costs, and benefits of adopting an emerging technology in a particular setting. although several studies have attempted to perform such investigations, these approaches adopt a qualitative data collection methodology which is limited in terms of the size of the targeted participant group and is associated with a significant manual overhead when transcribing and inferring results. this paper presents a scalable and automated framework for tracking likely adoption and/or rejection of new technologies from a large landscape of adopters. in particular, a large corpus of social media texts containing references to emerging technologies was compiled. text mining techniques were applied to extract sentiments expressed towards technology aspects. in the context of the problem definition herein, we hypothesise that the expression of positive sentiment infers an increase in the likelihood of impacting a technology user's acceptance to adopt, integrate, and/or use the technology, and negative sentiment infers an increase in the likelihood of impacting the rejection of emerging technologies by adopters. to quantitatively test our hypothesis, a ground truth analysis was performed to validate that the sentiment captured by the text mining approach is comparable to the results given by human annotators when asked to label whether such texts positively or negatively impact their outlook towards adopting an emerging technology.",2024-01-16
"building contextual knowledge graphs for personalized learning recommendations using text mining and semantic graph completion","hasan abu-rasheed, mareike dornh√∂fer, christian weber, g√°bor kismih√≥k, ulrike buchmann, madjid fathi","information retrieval","modelling learning objects (lo) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. while hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of los in those platforms. this leads to a foundation for personalized recommendations of learning paths. in this paper, the transformation of hierarchical data models into knowledge graph (kg) models of los using text mining is introduced and evaluated. we utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. we evaluate the kg structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. the results show that the relations in the kg are semantically comparable to those defined by domain experts, and that the proposed kg improves representing and linking the contexts of los through increasing graph communities and betweenness centrality.",2024-01-24
"topic modelling: going beyond token outputs","lowri williams, eirini anthi, laura arman, pete burnap","computation and language","topic modelling is a text mining technique for identifying salient themes from a number of documents. the output is commonly a set of topics consisting of isolated tokens that often co-occur in such documents. manual effort is often associated with interpreting a topic's description from such tokens. however, from a human's perspective, such outputs may not adequately provide enough information to infer the meaning of the topics; thus, their interpretability is often inaccurately understood. although several studies have attempted to automatically extend topic descriptions as a means of enhancing the interpretation of topic models, they rely on external language sources that may become unavailable, must be kept up-to-date to generate relevant results, and present privacy issues when training on or processing data. this paper presents a novel approach towards extending the output of traditional topic modelling methods beyond a list of isolated tokens. this approach removes the dependence on external sources by using the textual data itself by extracting high-scoring keywords and mapping them to the topic model's token outputs. to measure the interpretability of the proposed outputs against those of the traditional topic modelling approach, independent annotators manually scored each output based on their quality and usefulness, as well as the efficiency of the annotation task. the proposed approach demonstrated higher quality and usefulness, as well as higher efficiency in the annotation task, in comparison to the outputs of a traditional topic modelling method, demonstrating an increase in their interpretability.",2024-01-16
"examining the role of peer acknowledgements on social annotations: unraveling the psychological underpinnings","xiaoshan huang, haolun wu, xue liu, susanne lajoie","human-computer interaction","this study explores the impact of peer acknowledgement on learner engagement and implicit psychological attributes in written annotations on an online social reading platform. participants included 91 undergraduates from a large north american university. using log file data, we analyzed the relationship between learners' received peer acknowledgement and their subsequent annotation behaviours using cross-lag regression. higher peer acknowledgements correlate with increased initiation of annotations and responses to peer annotations. by applying text mining techniques and calculating shapley values to analyze 1,969 social annotation entries, we identified prominent psychological themes within three dimensions (i.e., affect, cognition, and motivation) that foster peer acknowledgment in digital social annotation. these themes include positive affect, openness to learning and discussion, and expression of motivation. the findings assist educators in improving online learning communities and provide guidance to technology developers in designing effective prompts, drawing from both implicit psychological cues and explicit learning behaviours.",2024-01-23
"biofinbert: finetuning large language models (llms) to analyze sentiment of press releases and financial text around inflection points of biotech stocks","valentina aparicio, daniel gordon, sebastian g. huayamares, yuhuai luo","general finance","large language models (llms) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. developing and training a new llm can be very computationally expensive, so it is becoming a common practice to take existing llms and finetune them with carefully curated datasets for desired applications in different fields. here, we present biofinbert, a finetuned llm to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. the stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. these clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. in our attempt to design a llm capable of analyzing the sentiment of these press releases,we first finetuned biobert, a biomedical language representation model designed for biomedical text mining, using financial textual databases. our finetuned model, termed biofinbert, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.",2024-01-19
"taec: a manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature","claire n√©dellec, clara sauvion, robert bossy, mariya borovikova, louise del√©ger","computation and language","wheat varieties show a large diversity of traits and phenotypes. linking them to genetic variability is essential for shorter and more efficient wheat breeding programs. newly desirable wheat variety traits include disease resistance to reduce pesticide use, adaptation to climate change, resistance to heat and drought stresses, or low gluten content of grains. wheat breeding experiments are documented by a large body of scientific literature and observational data obtained in-field and under controlled conditions. the cross-referencing of complementary information from the literature and observational data is essential to the study of the genotype-phenotype relationship and to the improvement of wheat selection. the scientific literature on genetic marker-assisted selection describes much information about the genotype-phenotype relationship. however, the variety of expressions used to refer to traits and phenotype values in scientific articles is a hinder to finding information and cross-referencing it. when trained adequately by annotated examples, recent text mining methods perform highly in named entity recognition and linking in the scientific domain. while several corpora contain annotations of human and animal phenotypes, currently, no corpus is available for training and evaluating named entity recognition and entity-linking methods in plant phenotype literature. the triticum aestivum trait corpus is a new gold standard for traits and phenotypes of wheat. it consists of 540 pubmed references fully annotated for trait, phenotype, and species named entities using the wheat trait and phenotype ontology and the species taxonomy of the national center for biotechnology information. a study of the performance of tools trained on the triticum aestivum trait corpus shows that the corpus is suitable for the training and evaluation of named entity recognition and linking.",2024-01-15
"two directions for clinical data generation with large language models: data-to-label and label-to-data","rumeng li, xun wang, hong yu","computation and language","large language models (llms) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored. we investigate whether llms can augment clinical data for detecting alzheimer's disease (ad)-related signs and symptoms from electronic health records (ehrs), a challenging task that requires high expertise. we create a novel pragmatic taxonomy for ad sign and symptom progression based on expert knowledge, which guides llms to generate synthetic data following two different directions: ""data-to-label"", which labels sentences from a public ehr collection with ad-related signs and symptoms; and ""label-to-data"", which generates sentences with ad-related signs and symptoms based on the label definition. we train a system to detect ad-related signs and symptoms from ehrs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal ehrs of ad patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method. we find that using the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. this shows that llms can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.",2023-12-09
"exact representation and efficient approximations of linear model predictive control laws via hardtanh type deep neural networks","daniela lupu, ion necoara","optimization and control","deep neural networks have revolutionized many fields, including image processing, inverse problems, text mining and more recently, give very promising results in systems and control. neural networks with hidden layers have a strong potential as an approximation framework of predictive control laws as they usually yield better approximation quality and smaller memory requirements than existing explicit (multi-parametric) approaches. in this paper, we first show that neural networks with hardtanh activation functions can exactly represent predictive control laws of linear time-invariant systems. we derive theoretical bounds on the minimum number of hidden layers and neurons that a hardtanh neural network should have to exactly represent a given predictive control law. the choice of hardtanh deep neural networks is particularly suited for linear predictive control laws as they usually require less hidden layers and neurons than deep neural networks with relu units for representing exactly continuous piecewise affine (or equivalently min-max) maps. in the second part of the paper we bring the physics of the model and standard optimization techniques into the architecture design, in order to eliminate the disadvantages of the black-box hardtanh learning. more specifically, we design trainable unfolded hardtanh deep architectures for learning linear predictive control laws based on two standard iterative optimization algorithms, i.e., projected gradient descent and accelerated projected gradient descent. we also study the performance of the proposed hardtanh type deep neural networks on a linear model predictive control application.",2024-01-10
"idofew: intermediate training using dual-clustering in language models for few labels text classification","abdullah alsuhaibani, hamad zogan, imran razzak, shoaib jameel, guandong xu","computation and language","language models such as bidirectional encoder representations from transformers (bert) have been very effective in various natural language processing (nlp) and text mining tasks including text classification. however, some tasks still pose challenges for these models, including text classification with limited labels. this can result in a cold-start problem. although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms. to overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors. the key novelty in our model, idofew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning. our approach has shown significant improvements compared to strong comparative models.",2024-01-08
"text mining arxiv: a look through quantitative finance papers","michele leonardo bianchi","digital libraries","this paper explores articles hosted on the arxiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arxiv from 1997 to 2022. we extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.",2024-01-03
"language model as an annotator: unsupervised context-aware quality phrase generation","zhihao zhang, yuan zuo, chenghua lin, junjie wu","computation and language","phrase mining is a fundamental text mining task that aims to identify quality phrases from context. nevertheless, the scarcity of extensive gold labels datasets, demanding substantial annotation efforts from experts, renders this task exceptionally challenging. furthermore, the emerging, infrequent, and domain-specific nature of quality phrases presents further challenges in dealing with this task. in this paper, we propose lmphrase, a novel unsupervised context-aware quality phrase mining framework built upon large pre-trained language models (lms). specifically, we first mine quality phrases as silver labels by employing a parameter-free probing technique called perturbed masking on the pre-trained language model bert (coined as annotator). in contrast to typical statistic-based or distantly-supervised methods, our silver labels, derived from large pre-trained language models, take into account rich contextual information contained in the lms. as a result, they bring distinct advantages in preserving informativeness, concordance, and completeness of quality phrases. secondly, training a discriminative span prediction model heavily relies on massive annotated data and is likely to face the risk of overfitting silver labels. alternatively, we formalize phrase tagging task as the sequence generation problem by directly fine-tuning on the sequence-to-sequence pre-trained language model bart with silver labels (coined as generator). finally, we merge the quality phrases from both the annotator and generator as the final predictions, considering their complementary nature and distinct characteristics. extensive experiments show that our lmphrase consistently outperforms all the existing competitors across two different granularity phrase mining tasks, where each task is tested on two different domain datasets.",2023-12-28
"efficient high-quality clustering for large bipartite graphs","renchi yang, jieming shi","social and information networks","a bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins. k-bipartite graph clustering (k-bgc) is to partition the target vertex set in a bipartite graph into k disjoint clusters. the clustering quality is important to the utility of k-bgc in various applications like social network analysis, recommendation systems, text mining, and bioinformatics, to name a few. existing approaches to k-bgc either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges. motivated by this, this paper presents two efficient k-bgc solutions, hope and hope+, which achieve state-of-the-art performance on large-scale bipartite graphs. hope obtains high scalability and effectiveness through a new k-bgc problem formulation based on the novel notion of high-order perspective (hop) vectors and an efficient technique for low-rank approximation of hop vectors. hope+ further elevates the k-bgc performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework. two variants, hope+ (fnem) and hope+ (snem) are designed when either the frobenius norm or spectral norm is applied in the transformation. extensive experiments, comparing hope and hope+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially hope+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster. on the largest dataset mag with 1.1 billion edges, hope+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for k-bgc.",2023-12-28
"debiasing sample loadings and scores in exponential family pca for sparse count data","ruochen huang, yoonkyung lee","methodology","multivariate count data with many zeros frequently occur in a variety of application areas such as text mining with a document-term matrix and cluster analysis with microbiome abundance data. exponential family pca (collins et al., 2001) is a widely used dimension reduction tool to understand and capture the underlying low-rank structure of count data. it produces principal component scores by fitting poisson regression models with estimated loadings as covariates. this tends to result in extreme scores for sparse count data significantly deviating from true scores. we consider two major sources of bias in this estimation procedure and propose ways to reduce their effects. first, the discrepancy between true loadings and their estimates under a limited sample size largely degrades the quality of score estimates. by treating estimated loadings as covariates with bias and measurement errors, we debias score estimates, using the iterative bootstrap method for loadings and considering classical measurement error models. second, the existence of mle bias is often ignored in score estimation, but this bias could be removed through well-known mle bias reduction methods. we demonstrate the effectiveness of the proposed bias correction procedure through experiments on both simulated data and real data.",2023-12-20
"wikimute: a web-sourced dataset of semantic descriptions for music audio","benno weck, holger kirchhoff, peter grosche, xavier serra","computation and language","multi-modal deep learning techniques for matching free-form text with music have shown promising results in the field of music information retrieval (mir). prior work is often based on large proprietary data while publicly available datasets are few and small in size. in this study, we present wikimute, a new and open dataset containing rich semantic descriptions of music. the data is sourced from wikipedia's rich catalogue of articles covering musical works. using a dedicated text-mining pipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo. to show the use of this data, we train a model that jointly learns text and audio representations and performs cross-modal retrieval. the model is evaluated on two tasks: tag-based music retrieval and music auto-tagging. the results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training.",2023-12-14
"uncovering gender stereotypes in video game character designs: a multi-modal analysis of honor of kings","bingqing liu, kyrie zhixuan zhou, danlei zhu, jaihyun park","human-computer interaction","in this paper, we conduct a comprehensive analysis of gender stereotypes in the character design of honor of kings, a popular multiplayer online battle arena (moba) game in china. we probe gender stereotypes through the lens of role assignments, visual designs, spoken lines, and background stories, combining qualitative analysis and text mining based on the moral foundation theory. male heroes are commonly designed as masculine fighters with power and female heroes as feminine ""ornaments"" with ideal looks. we contribute with a culture-aware and multi-modal understanding of gender stereotypes in games, leveraging text-, visual-, and role-based evidence.",2023-11-23
"taiyi: a bilingual fine-tuned large language model for diverse biomedical tasks","ling luo, jinzhong ning, yingwen zhao, zhijun wang, zeyuan ding, peng chen, weiru fu, qinyu han, guangtao xu, yunzhi qiu, dinghao pan, jiru li, hao li, wenduo feng, senbo tu, yuqi liu, zhihao yang, jian wang, yuanyuan sun, hongfei lin","computation and language","objective: most existing fine-tuned biomedical large language models (llms) focus on enhancing performance in monolingual biomedical question answering and conversation tasks. to investigate the effectiveness of the fine-tuned llms on diverse biomedical nlp tasks in different languages, we present taiyi, a bilingual fine-tuned llm for diverse biomedical tasks. materials and methods: we first curated a comprehensive collection of 140 existing biomedical text mining datasets (102 english and 38 chinese datasets) across over 10 task types. subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. results: experimental results on 13 test sets covering named entity recognition, relation extraction, text classification, question answering tasks demonstrate that taiyi achieves superior performance compared to general llms. the case study involving additional biomedical nlp tasks further shows taiyi's considerable potential for bilingual biomedical multi-tasking. conclusion: leveraging rich high-quality biomedical corpora and developing effective fine-tuning strategies can significantly improve the performance of llms within the biomedical domain. taiyi shows the bilingual multi-tasking capability through supervised fine-tuning. however, those tasks such as information extraction that are not generation tasks in nature remain challenging for llm-based generative approaches, and they still underperform the conventional discriminative approaches of smaller language models.",2023-11-20
"a comprehensive review on sentiment analysis: tasks, approaches and applications","sudhanshu kumar (1), partha pratim roy (1), debi prosad dogra (2), byung-gyu kim (3) ((1) department of computer science and engineering, iit roorkee, india, (2) school of electrical sciences, iit bhubaneswar, odisha, india, (3) department of it engineering, sookmyung women's university, seoul, south korea)","artificial intelligence","sentiment analysis (sa) is an emerging field in text mining. it is the process of computationally identifying and categorizing opinions expressed in a piece of text over different social media platforms. social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. most organizations depend on the customer's response and feedback to upgrade their offered products and services. sa or opinion mining seems to be a promising research area for various domains. it plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. this survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, and text. the challenges and opportunities of sentiment analysis are also discussed in the paper. \keywords{sentiment analysis, machine learning, lexicon-based approach, deep learning, natural language processing}",2023-11-19
"insights into classifying and mitigating llms' hallucinations","alessandro bruno, pier luigi mazzeo, aladine chetouani, marouane tliba, mohamed amine kerkouri","computation and language","the widespread adoption of large language models (llms) across diverse ai applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. however, llms are not exempt from drawbacks. one of the most concerning aspects regards the emerging problematic phenomena known as ""hallucinations"". they manifest in text generation systems, particularly in question-answering systems reliant on llms, potentially resulting in false or misleading information propagation. this paper delves into the underlying causes of ai hallucination and elucidates its significance in artificial intelligence. in particular, hallucination classification is tackled over several tasks (machine translation, question and answer, dialog systems, summarisation systems, knowledge graph with llms, and visual question answer). additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of llms. our research addresses this critical issue within the herefanmi (health-related fake news mitigation) project, generously supported by ngi search, dedicated to combating health-related fake news dissemination on the internet. this endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving ai technologies.",2023-11-14
"bidrn: a method of bidirectional recurrent neural network for sentiment analysis","dr. d muthusankar, dr. p kaladevi, dr. v r sadasivam, r praveen","computation and language","text mining research has grown in importance in recent years due to the tremendous increase in the volume of unstructured textual data. this has resulted in immense potential as well as obstacles in the sector, which may be efficiently addressed with adequate analytical and study methods. deep bidirectional recurrent neural networks are used in this study to analyze sentiment. the method is categorized as sentiment polarity analysis because it may generate a dataset with sentiment labels. this dataset can be used to train and evaluate sentiment analysis models capable of extracting impartial opinions. this paper describes the sentiment analysis-deep bidirectional recurrent neural networks (sa-bdrnn) scheme, which seeks to overcome the challenges and maximize the potential of text mining in the context of big data. the current study proposes a sa-dbrnn scheme that attempts to give a systematic framework for sentiment analysis in the context of student input on institution choice. the purpose of this study is to compare the effectiveness of the proposed sa- dbrnn scheme to existing frameworks to establish a robust deep neural network that might serve as an adequate classification model in the field of sentiment analysis.",2023-11-13
"matnexus: a comprehensive text mining and analysis suite for materials discover","lei zhang, markus stricker","materials science","matnexus is a specialized software for the automated collection, processing, and analysis of text from scientific articles. through an integrated suite of modules, the matnexus facilitates the retrieval of scientific articles, processes textual data for insights, generates vector representations suitable for machine learning, and offers visualization capabilities for word embeddings. with the vast volume of scientific publications, matnexus stands out as an end-to-end tool for researchers aiming to gain insights from scientific literature in material science, making the exploration of materials, such as the electrocatalyst examples we show here, efficient and insightful.",2023-11-07
"automated annotation of scientific texts for ml-based keyphrase extraction and validation","oluwamayowa o. amusat, harshad hegde, christopher j. mungall, anna giannakou, neil p. byers, dan gunter, kjiersten fagnan, lavanya ramakrishnan","information retrieval","advanced omics technologies and facilities generate a wealth of valuable data daily; however, the data often lacks the essential metadata required for researchers to find and search them effectively. the lack of metadata poses a significant challenge in the utilization of these datasets. machine learning-based metadata extraction techniques have emerged as a potentially viable approach to automatically annotating scientific datasets with the metadata necessary for enabling effective search. text labeling, usually performed manually, plays a crucial role in validating machine-extracted metadata. however, manual labeling is time-consuming; thus, there is an need to develop automated text labeling techniques in order to accelerate the process of scientific innovation. this need is particularly urgent in fields such as environmental genomics and microbiome science, which have historically received less attention in terms of metadata curation and creation of gold-standard text mining datasets. in this paper, we present two novel automated text labeling approaches for the validation of ml-generated metadata for unlabeled texts, with specific applications in environmental genomics. our techniques show the potential of two new ways to leverage existing information about the unlabeled texts and the scientific domain. the first technique exploits relationships between different types of data sources related to the same research study, such as publications and proposals. the second technique takes advantage of domain-specific controlled vocabularies or ontologies. in this paper, we detail applying these approaches for ml-generated metadata validation. our results show that the proposed label assignment approaches can generate both generic and highly-specific text labels for the unlabeled texts, with up to 44% of the labels matching with those suggested by a ml keyword extraction algorithm.",2023-11-08
"in nomine patris... elements for a semantics of medieval paternity","nicolas perreaux (lamop)","digital libraries","this article examines medieval concepts of paternity and father-son relationships through the digital analysis of medieval textual corpora. although historians have access to enormous digital collections in 2023, they have rarely fully exploited these resources. the author proposes a historical semantic approach to this theme, using modeling tools and text mining in general, to analyze the evolution of terms related to paternity. the study proposes three conclusions: 1. a semantic break occurred in the semantic field of paternity at the turn of antiquity and the early middle ages. the meaning of pater and its derivatives changed radically over the course of the 4th-6th centuries, particularly as a result of the influence of the dogma of the christian trinity. medieval fatherhood was multidimensional, encompassing both biological and spiritual aspects, in other words, complex relationships between multiple carnal and spiritual (i.e. divine) fathers. 2. the role of spiritual kinship is crucial to understanding medieval fatherhood, as the work of anita guerreau-jalabert and j{√©}r{√¥}me baschet has already shown. initially attributed to god, this ''ideal paternity'' (paternitas) gradually extended to members of the church (popes, bishops, abbots), underlining at the same time the growing importance of spiritual kinship over biological kinship over the centuries studied. 3. to reveal these structures, invisible to the naked eye, an interdisciplinary approach is rigorously required. complementary investigations into the lemmas mater, filia, frater and other family terms are required. the use of digital tools and historical semantic analysis opens up new perspectives for researchers in history, anthropology, linguistics and data mining, enabling them to explore the representation systems of ancient societies in depth and with nuance.",2023-10-06
"dail: data augmentation for in-context learning via self-paraphrase","dawei li, yaxuan li, dheeraj mekala, shuyao li, yulin wang, xueqi wang, william hogan, jingbo shang","computation and language","in-context learning (icl) combined with pre-trained large language models has achieved promising results on various nlp tasks. however, icl requires high-quality annotated demonstrations which might not be available in real-world scenarios. to overcome this limitation, we propose \textbf{d}ata \textbf{a}ugmentation for \textbf{i}n-context \textbf{l}earning (\textbf{dail}). dail leverages the intuition that large language models are more familiar with the content generated by themselves. it first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. our extensive empirical evaluation shows that dail outperforms the standard icl method and other ensemble-based methods in the low-resource scenario. additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. we believe our work will stimulate further research on icl in low-resource settings.",2023-11-06
"content significance distribution of sub-text blocks in articles and its application to article-organization assessment","you zhou, jie wang","computer vision and pattern recognition","we explore how to capture the significance of a sub-text block in an article and how it may be used for text mining tasks. a sub-text block is a sub-sequence of sentences in the article. we formulate the notion of content significance distribution (csd) of sub-text blocks, referred to as csd of the first kind and denoted by csd-1. in particular, we leverage hugging face's sentencetransformer to generate contextual sentence embeddings, and use moverscore over text embeddings to measure how similar a sub-text block is to the entire text. to overcome the exponential blowup on the number of sub-text blocks, we present an approximation algorithm and show that the approximated csd-1 is almost identical to the exact csd-1. under this approximation, we show that the average and median csd-1's for news, scholarly research, argument, and narrative articles share the same pattern. we also show that under a certain linear transformation, the complement of the cumulative distribution function of the beta distribution with certain values of $\alpha$ and $\beta$ resembles a csd-1 curve. we then use csd-1's to extract linguistic features to train an svc classifier for assessing how well an article is organized. through experiments, we show that this method achieves high accuracy for assessing student essays. moreover, we study csd of sentence locations, referred to as csd of the second kind and denoted by csd-2, and show that average csd-2's for different types of articles possess distinctive patterns, which either conform common perceptions of article structures or provide rectification with minor deviation.",2023-11-03
"an energy-based comparative analysis of common approaches to text classification in the legal domain","sinan gultekin, achille globo, andrea zugarini, marco ernandes, leonardo rigutini","computation and language","most machine learning research evaluates the best solutions in terms of performance. however, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. in fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. large language models (llms) are extensively adopted to address nlp problems in academia and industry. in this work, we present a detailed quantitative comparison of llm and traditional approaches (e.g. svm) on the lexglue benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. in our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they follow different implementation procedures and also require different resources. the results indicate that very often, the simplest algorithms achieve performance very close to that of large llms but with very low power consumption and lower resource demands. the results obtained could suggest companies to include additional evaluations in the choice of machine learning (ml) solutions.",2023-11-02
"sentence bag graph formulation for biomedical distant supervision relation extraction","hao zhang, yang liu, xiaoyan liu, tianming liang, gaurav sharma, liang xue, maozu guo","machine learning","we introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. the proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. extensive experiments on two large-scale biomedical relation datasets and the widely utilized nyt dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the general text mining domain.",2023-10-29
"test-time augmentation for factual probing","go kamoda, benjamin heinzerling, keisuke sakaguchi, kentaro inui","computation and language","factual probing is a method that uses prompts to test if a language model ""knows"" certain world knowledge facts. a problem in factual probing is that small changes to the prompt can lead to large changes in model output. previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. however, such approaches are relation-specific and do not generalize to unseen relation types. here, we propose to use test-time augmentation (tta) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. experiments show improved model calibration, i.e., with tta, model confidence better reflects prediction accuracy. improvements in prediction accuracy are observed for some models, but for other models, tta leads to degradation. error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for tta.",2023-10-26
"unraveling the skillsets of data scientists: text mining analysis of dutch university master programs in data science and artificial intelligence","mathijs j. mol, barbara belfi, zsuzsa bakk","other statistics","the growing demand for data scientists in the global labor market and the netherlands has led to a rise in data science and artificial intelligence (ai) master programs offered by universities. however, there is still a lack of clarity regarding the specific skillsets of data scientists. this study aims to address this issue by employing correlated topic modeling (ctm) to analyse the content of 41 master programs offered by seven dutch universities. we assess the differences and similarities in the core skills taught by these programs, determine the subject-specific and general nature of the skills, and provide a comparison between the different types of universities offering these programs. our findings reveal that research, data processing, statistics and ethics are the predominant skills taught in dutch data science and ai master programs, with general universities emphasizing research skills and technical universities focusing more on it and electronic skills. this study contributes to a better understanding of the diverse skillsets of data scientists, which is essential for employers, universities, and prospective students.",2023-10-23
"the uncertainty-based retrieval framework for ancient chinese cws and pos","pengyu wang, zhichen ren","computation and language","automatic analysis for modern chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient chinese is still relatively rare. ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. in this paper, we propose a framework for ancient chinese word segmentation and part-of-speech tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. the performance of our architecture outperforms pre-trained bert with crf and existing tools such as jiayan.",2023-10-12
"an analysis on large language models in healthcare: a case study of biobert","shyni sharaf, v. s. anoop","artificial intelligence","this paper conducts a comprehensive investigation into applying large language models, particularly on biobert, in healthcare. it begins with thoroughly examining previous natural language processing (nlp) approaches in healthcare, shedding light on the limitations and challenges these methods face. following that, this research explores the path that led to the incorporation of biobert into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. the analysis outlines a systematic methodology for fine-tuning biobert to meet the unique needs of the healthcare domain. this approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. additionally, the paper covers aspects related to model evaluation, with a focus on healthcare benchmarks and functions like processing of natural language in biomedical, question-answering, clinical document classification, and medical entity recognition. it explores techniques to improve the model's interpretability and validates its performance compared to existing healthcare-focused language models. the paper thoroughly examines ethical considerations, particularly patient privacy and data security. it highlights the benefits of incorporating biobert into healthcare contexts, including enhanced clinical decision support and more efficient information retrieval. nevertheless, it acknowledges the impediments and complexities of this integration, encompassing concerns regarding data privacy, transparency, resource-intensive requirements, and the necessity for model customization to align with diverse healthcare domains.",2023-10-11
"foreseer: product aspect forecasting using temporal graph embedding","zixuan liu, gaurush hiranandani, kun qian, eddie w. huang, yi xu, belinda zeng, karthik subbian, sheng wang","information retrieval","developing text mining approaches to mine aspects from customer reviews has been well-studied due to its importance in understanding customer needs and product attributes. in contrast, it remains unclear how to predict the future emerging aspects of a new product that currently has little review information. this task, which we named product aspect forecasting, is critical for recommending new products, but also challenging because of the missing reviews. here, we propose foreseer, a novel textual mining and product embedding approach progressively trained on temporal product graphs for this novel product aspect forecasting task. foreseer transfers reviews from similar products on a large product graph and exploits these reviews to predict aspects that might emerge in future reviews. a key novelty of our method is to jointly provide review, product, and aspect embeddings that are both time-sensitive and less affected by extremely imbalanced aspect frequencies. we evaluated foreseer on a real-world product review system containing 11,536,382 reviews and 11,000 products over 3 years. we observe that foreseer substantially outperformed existing approaches with at least 49.1\% auprc improvement under the real setting where aspect associations are not given. foreseer further improves future link prediction on the product graph and the review aspect association prediction. collectively, foreseer offers a novel framework for review forecasting by effectively integrating review text, product network, and temporal information, opening up new avenues for online shopping recommendation and e-commerce applications.",2023-10-07
"literature based discovery (lbd): towards hypothesis generation and knowledge discovery in biomedical text mining","balu bhasuran, gurusamy murugesan, jeyakumar natarajan","information retrieval","biomedical knowledge is growing in an astounding pace with a majority of this knowledge is represented as scientific publications. text mining tools and methods represents automatic approaches for extracting hidden patterns and trends from this semi structured and unstructured data. in biomedical text mining, literature based discovery (lbd) is the process of automatically discovering novel associations between medical terms otherwise mentioned in disjoint literature sets. lbd approaches proven to be successfully reducing the discovery time of potential associations that are hidden in the vast amount of scientific literature. the process focuses on creating concept profiles for medical terms such as a disease or symptom and connecting it with a drug and treatment based on the statistical significance of the shared profiles. this knowledge discovery approach introduced in 1989 still remains as a core task in text mining. currently the abc principle based two approaches namely open discovery and closed discovery are mostly explored in lbd process. this review starts with general introduction about text mining followed by biomedical text mining and introduces various literature resources such as medline, umls, mesh, and semmeddb. this is followed by brief introduction of the core abc principle and its associated two approaches open discovery and closed discovery in lbd process. this review also discusses the deep learning applications in lbd by reviewing the role of transformer models and neural networks based lbd models and its future aspects. finally, reviews the key biomedical discoveries generated through lbd approaches in biomedicine and conclude with the current limitations and future directions of lbd.",2023-10-04
"procedural text mining with large language models","anisa rula, jennifer d'souza","computation and language","recent advancements in the field of natural language processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of knowledge engineering. in this paper, we investigate the usage of large language models (llms) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured pdf text in an incremental question-answering fashion. in particular, we leverage the current state-of-the-art gpt-4 (generative pre-trained transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. the findings highlight both the promise of this approach and the value of the in-context learning customisations. these modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based natural language processing techniques for procedure extraction.",2023-10-05
"a review of undergraduate courses in design of experiments offered by american universities","alan r. vazquez, xiaocong xuan","other statistics","design of experiments (doe) is a relevant class to undergraduate programs in the sciences, because it teaches students how to plan, conduct, and analyze experiments. in the literature on doe, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software for constructing experimental designs. however, there are virtually no systematic assessments of the actual doe pedagogy. to address this issue, we build the first database of undergraduate doe courses offered in the united states of america. the database has records on courses offered from 2019 to 2022 by the best universities in the us news best national universities ranking of 2022. specifically, it has data on 18 general and content-specific features of 206 courses. to study the doe pedagogy, we analyze the database using descriptive statistics and text mining. our main findings include that most undergraduate doe courses follow the textbook ""design of and analysis of experiments"" by douglas montgomery, use the r software, and emphasize the learning of multifactor designs, randomization restrictions, data analysis, and applications. based on our analysis, we provide instructors with recommendations and teaching material to enhance their doe courses. the database and material are included in the supplementary material.",2023-09-29
"machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence","andres karjus","computation and language","the increasing capacities of large language models (llms) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. this contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. in contrast to the focus on english in the emerging llm applicability literature, many examples here deal with scenarios involving smaller languages and historical texts prone to digitization distortions. in all but the most difficult tasks requiring expert knowledge, generative llms can demonstrably serve as viable research instruments. llm (and human) annotations may contain errors and variation, but the agreement rate can and should be accounted for in subsequent statistical modeling; a bootstrapping approach is discussed. the replications among the case studies illustrate how tasks previously requiring potentially months of team effort and complex computational pipelines, can now be accomplished by an llm-assisted scholar in a fraction of the time. importantly, this approach is not intended to replace, but to augment researcher knowledge and skills. with these opportunities in sight, qualitative expertise and the ability to pose insightful questions have arguably never been more critical.",2023-09-24
"graph representation learning towards patents network analysis","mohammad heydari, babak teimourpour","social and information networks","patent analysis has recently been recognized as a powerful technique for large companies worldwide to lend them insight into the age of competition among various industries. this technique is considered a shortcut for developing countries since it can significantly accelerate their technology development. therefore, as an inevitable process, patent analysis can be utilized to monitor rival companies and diverse industries. this research employed a graph representation learning approach to create, analyze, and find similarities in the patent data registered in the iranian official gazette. the patent records were scrapped and wrangled through the iranian official gazette portal. afterward, the key entities were extracted from the scrapped patents dataset to create the iranian patents graph from scratch based on novel natural language processing and entity resolution techniques. finally, thanks to the utilization of novel graph algorithms and text mining methods, we identified new areas of industry and research from iranian patent data, which can be used extensively to prevent duplicate patents, familiarity with similar and connected inventions, awareness of legal entities supporting patents and knowledge of researchers and linked stakeholders in a particular research field.",2023-09-25
"multiple relations classification using imbalanced predictions adaptation","sakher khalil alqaaidi, elika bozorgi, krzysztof j. kochut","computation and language","the relation classification task assigns the proper semantic relation to a pair of subject and object entities; the task plays a crucial role in various text mining applications, such as knowledge graph construction and entities interaction discovery in biomedical text. current relation classification models employ additional procedures to identify multiple relations in a single sentence. furthermore, they overlook the imbalanced predictions pattern. the pattern arises from the presence of a few valid relations that need positive labeling in a relatively large predefined relations set. we propose a multiple relations classification model that tackles these issues through a customized output architecture and by exploiting additional input features. our findings suggest that handling the imbalanced predictions leads to significant improvements, even on a modest training design. the results demonstrate superiority performance on benchmark datasets commonly used in relation classification. to the best of our knowledge, this work is the first that recognizes the imbalanced predictions within the relation classification task.",2023-09-24
"decoding the alphabet soup of degrees in the united states postsecondary education system through hybrid method: database and text mining","sahar voghoei, james byars, john a miller, khaled rasheed, hamid a arabnia","information retrieval","this paper proposes a model to predict the levels (e.g., bachelor, master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the national student clearinghouse (nsc). the model will be the hybrid of two modules. the first module interprets the relevant abbreviatory elements embedded in nsc reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by american postsecondary educators. the second module is a combination of feature classification and text mining modeled with cnn-bilstm, which is preceded by several steps of heavy pre-processing. the model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. to date, such a classification strategy has not been attempted except using manual methods and simple text parsing logic.",2023-09-06
"computational approaches for predicting drug-disease associations: a comprehensive review","chunyan ao, zhichao xiao, lixin guan, liang yu","quantitative methods","in recent decades, traditional drug research and development have been facing challenges such as high cost, long timelines, and high risks. to address these issues, many computational approaches have been suggested for predicting the relationship between drugs and diseases through drug repositioning, aiming to reduce the cost, development cycle, and risks associated with developing new drugs. researchers have explored different computational methods to predict drug-disease associations, including drug side effects-disease associations, drug-target associations, and mirnadisease associations. in this comprehensive review, we focus on recent advances in predicting drug-disease association methods for drug repositioning. we first categorize these methods into several groups, including neural network-based algorithms, matrixbased algorithms, recommendation algorithms, link-based reasoning algorithms, and text mining and semantic reasoning. then, we compare the prediction performance of existing drug-disease association prediction algorithms. lastly, we delve into the present challenges and future prospects concerning drug-disease associations.",2023-09-10
"the power of patents: leveraging text mining and social network analysis to forecast iot trends","mehrdad maghsoudi, reza nourbakhsh, mehrdad agha mohammadali kermani, rahim khanizad","social and information networks","technology has become an indispensable competitive tool as science and technology have progressed throughout history. organizations can compete on an equal footing by implementing technology appropriately. technology trends or technology lifecycles begin during the initiation phase. finally, it reaches saturation after entering the maturity phase. as technology reaches saturation, it will be removed or replaced by another. this makes investing in technologies during this phase unjustifiable. technology forecasting is a critical tool for research and development to determine the future direction of technology. based on registered patents, this study examined the trends of iot technologies. a total of 3697 patents related to the internet of things from the last six years of patenting have been gathered using this http url for this purpose. the main people and companies were identified through the creation of the iot patent registration cooperation network, and the main groups active in patent registration were identified by the community detection technique. the patents were then divided into six technology categories: safety and security, information services, public safety and environment monitoring, collaborative aware systems, smart homes/buildings, and smart grid. and their technical maturity was identified and examined using the sigma plot program. based on the findings, information services technologies are in the saturation stage, while both smart homes/buildings, and smart grid technologies are in the saturation stage. three technologies, safety and security, public safety and environment monitoring, and collaborative aware systems are in the maturity stage.",2023-09-01
"insights into the nutritional prevention of macular degeneration based on a comparative topic modeling approach","lucas cassiel jacaruso","computation and language","topic modeling and text mining are subsets of natural language processing (nlp) with relevance for conducting meta-analysis (ma) and systematic review (sr). for evidence synthesis, the above nlp methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of sr and ma. instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. specifically, the objective is to identify topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence in (and consistency of distribution across) reports of significant effects. the proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (md). four of these were further supported in terms of effectiveness upon conducting a follow-up literature search for validation (omega-3 fatty acids, copper, zeaxanthin, and nitrates). the two not supported by the follow-up literature search (niacin and molybdenum) also had scores in the lowest range under the proposed scoring system, suggesting that the proposed methods score for a given topic may be a viable proxy for its degree of association with the outcome of interest and can be helpful in the search for potentially causal relationships. these results underpin the proposed methods potential to add specificity in understanding effects from broad-scope reports, elucidate topics of interest for future research, and guide evidence synthesis in a systematic and scalable way. all of this is accomplished while yielding valuable insights into the prevention of md.",2023-09-01
"biomedical entity linking with triple-aware pre-training","xi yan, cedric m√∂ller, ricardo usbeck","computation and language","linking biomedical entities is an essential aspect in biomedical natural language processing tasks, such as text mining and question answering. however, a difficulty of linking the biomedical entities using current large language models (llm) trained on a general corpus is that biomedical entities are scarcely distributed in texts and therefore have been rarely seen during training by the llm. at the same time, those llms are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts. to cope with aforementioned problems, some recent works focused on injecting knowledge graph information into llms. however, former methods either ignore the relational knowledge of the entities or lead to catastrophic forgetting. therefore, we propose a novel framework to pre-train the powerful generative llm by a corpus synthesized from a kg. in the evaluations we are unable to confirm the benefit of including synonym, description or relational information.",2023-08-28
"feature extraction using deep generative models for bangla text classification on a new comprehensive dataset","md. rafi-ur-rashid, sami azam, mirjam jonkman","information retrieval","the selection of features for text classification is a fundamental task in text mining and information retrieval. despite being the sixth most widely spoken language in the world, bangla has received little attention due to the scarcity of text datasets. in this research, we collected, annotated, and prepared a comprehensive dataset of 212,184 bangla documents in seven different categories and made it publicly accessible. we implemented three deep learning generative models: lstm variational autoencoder (lstm vae), auxiliary classifier generative adversarial network (ac-gan), and adversarial autoencoder (aae) to extract text features, although their applications are initially found in the field of computer vision. we utilized our dataset to train these three models and used the feature space obtained in the document classification task. we evaluated the performance of the classifiers and found that the adversarial autoencoder model produced the best feature space.",2023-08-21
"ds4dh at #smm4h 2023: zero-shot adverse drug events normalization using sentence transformers and reciprocal-rank fusion","anthony yazdani, hossein rouhizadeh, david vicente alvarez, douglas teodoro","computation and language","this paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the data science for digital health (ds4dh) group for the social media mining for health applications (smm4h) 2023 shared task 5. shared task 5 targeted the normalization of adverse drug event mentions in twitter to standard concepts of the medical dictionary for regulatory activities terminology. our system hinges on a two-stage approach: bert fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. the approach yielded a precision of 44.9%, recall of 40.5%, and an f1-score of 42.6%. it outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. these results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.",2023-08-15
"belb: a biomedical entity linking benchmark","samuele garda, leon weber-genzel, robert martin, ulf leser","computation and language","biomedical entity linking (bel) is the task of grounding entity mentions to a knowledge base. it plays a vital role in information extraction pipelines for the life sciences literature. we review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base umls, leaving their performance to more specialized ones, e.g. genes or variants, understudied. we therefore developed belb, a biomedical entity linking benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. belb greatly reduces preprocessing overhead in testing bel systems on multiple corpora offering a standardized testbed for reproducible experiments. using belb we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models. our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models.",2023-08-22
"economic policy uncertainty: a review on applications and measurement methods with focus on text mining methods","fatemeh kaveh-yazdy, sajjad zarifzadeh","computers and society","economic policy uncertainty (epu) represents the uncertainty realized by the investors during economic policy alterations. epu is a critical indicator in economic studies to predict future investments, the unemployment rate, and recessions. epu values can be estimated based on financial parameters directly or implied uncertainty indirectly using the text mining methods. although epu is a well-studied topic within the economy, the methods utilized to measure it are understudied. in this article, we define the epu briefly and review the methods used to measure the epu, and survey the areas influenced by the changes in epu level. we divide the epu measurement methods into three major groups with respect to their input data. examples of each group of methods are enlisted, and the pros and cons of the groups are discussed. among the epu measures, text mining-based ones are dominantly studied. these methods measure the realized uncertainty by taking into account the uncertainty represented in the news and publicly available sources of financial information. finally, we survey the research areas that rely on measuring the epu index with the hope that studying the impacts of uncertainty would attract further attention of researchers from various research fields. in addition, we propose a list of future research approaches focusing on measuring epu using textual material.",2023-08-20
"real-time construction algorithm of co-occurrence network based on inverted index","jiahao cheng","information retrieval","co-occurrence networks are an important method in the field of natural language processing and text mining for discovering semantic relationships within texts. however, the traditional traversal algorithm for constructing co-occurrence networks has high time complexity and space complexity when dealing with large-scale text data. in this paper, we propose an optimized algorithm based on inverted indexing and breadth-first search to improve the efficiency of co-occurrence network construction and reduce memory consumption. firstly, the traditional traversal algorithm is analyzed, and its performance issues in constructing co-occurrence networks are identified. then, the detailed implementation process of the optimized algorithm is presented. subsequently, the csl large-scale chinese scientific literature dataset is used for experimental validation, comparing the performance of the traditional traversal algorithm and the optimized algorithm in terms of running time and memory usage. finally, using non-parametric test methods, the optimized algorithm is proven to have significantly better performance than the traditional traversal algorithm. the research in this paper provides an effective method for the rapid construction of co-occurrence networks, contributing to the further development of the information organization fields.",2023-08-17
"bi-lava: biocuration with hierarchical image labeling through active learning and visual analysis","juan trelles, andrew wentzel, william berrios, g. elisabeta marai","human-computer interaction","in the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. however, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. from a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. we implement this strategy in a system called bi-lava biocuration with hierarchical image labeling through active learning and visual analysis. bi-lava leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images. bi-lava's front end uses custom encodings to represent data distributions, taxonomies, image projections, and neighborhoods of image thumbnails, which help model builders explore an unfamiliar image dataset and taxonomy and correct and generate labels. an evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labeled and unlabeled collections.",2023-08-15
"multi-modal multi-view clustering based on non-negative matrix factorization","yasser khalafaoui (alteca, etis - umr 8051, cy), nistor grozavu (etis - umr 8051, cy), basarab matei (lipn), laurent-walter goix","artificial intelligence","by combining related objects, unsupervised machine learning techniques aim to reveal the underlying patterns in a data set. non-negative matrix factorization (nmf) is a data mining technique that splits data matrices by imposing restrictions on the elements' non-negativity into two matrices: one representing the data partitions and the other to represent the cluster prototypes of the data set. this method has attracted a lot of attention and is used in a wide range of applications, including text mining, clustering, language modeling, music transcription, and neuroscience (gene separation). the interpretation of the generated matrices is made simpler by the absence of negative values. in this article, we propose a study on multi-modal clustering algorithms and present a novel method called multi-modal multi-view non-negative matrix factorization, in which we analyze the collaboration of several local nmf models. the experimental results show the value of the proposed approach, which was evaluated using a variety of data sets, and the obtained results are very promising compared to state of art methods.",2023-08-09
"biobert based snp-traits associations extraction from biomedical literature","mohammad dehghani, behrouz bokharaeian, zahra yazdanparast","computation and language","scientific literature contains a considerable amount of information that provides an excellent opportunity for developing text mining methods to extract biomedical relationships. an important type of information is the relationship between singular nucleotide polymorphisms (snp) and traits. in this paper, we present a biobert-gru method to identify snp- traits associations. based on the evaluation of our method on the snpphena dataset, it is concluded that this new method performs better than previous machine learning and deep learning based methods. biobert-gru achieved the result a precision of 0.883, recall of 0.882 and f1-score of 0.881.",2023-08-03
"industrial memories: exploring the findings of government inquiries with neural word embedding and machine learning","susan leavy, emilie pine, mark t keane","computation and language","we present a text mining system to support the exploration of large volumes of text detailing the findings of government inquiries. despite their historical significance and potential societal impact, key findings of inquiries are often hidden within lengthy documents and remain inaccessible to the general public. we transform the findings of the irish government's inquiry into industrial schools and through the use of word embedding, text classification and visualisation, present an interactive web-based platform that enables the exploration of the text to uncover new historical insights.",2023-08-02
"a short review of the main concerns in a.i. development and application within the public sector supported by nlp and tm","carlos ferreira","computers and society","artificial intelligence is not a new subject, and business, industry and public sectors have used it in different ways and contexts and considering multiple concerns. this work reviewed research papers published in acm digital library and ieee xplore conference proceedings in the last two years supported by fundamental concepts of natural language processing (nlp) and text mining (tm). the objective was to capture insights regarding data privacy, ethics, interpretability, explainability, trustworthiness, and fairness in the public sector. the methodology has saved analysis time and could retrieve papers containing relevant information. the results showed that fairness was the most frequent concern. the least prominent topic was data privacy (although embedded in most articles), while the most prominent was trustworthiness. finally, gathering helpful insights about those concerns regarding a.i. applications in the public sector was also possible.",2023-07-25
"supply chain emission estimation using large language models","ayush jain, manikandan padmanaban, jagabondhu hazra, shantanu godbole, kommy weldemariam","computation and language","large enterprises face a crucial imperative to achieve the sustainable development goals (sdgs), especially goal 13, which focuses on combating climate change and its impacts. to mitigate the effects of climate change, reducing enterprise scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. however, tracking scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream this http url address the above mentioned challenges, we propose a first-of-a-kind framework that uses domain-adapted nlp foundation models to estimate scope 3 emissions, by utilizing financial transactions as a proxy for purchased goods and services. we compared the performance of the proposed framework with the state-of-art text classification models such as tf-idf, word2vec, and zero shot learning. our results show that the domain-adapted foundation model outperforms state-of-the-art text mining techniques and performs as well as a subject matter expert (sme). the proposed framework could accelerate the scope 3 estimation at enterprise scale and will help to take appropriate climate actions to achieve sdg 13.",2023-08-03
"evaluating chatgpt text-mining of clinical records for obesity monitoring","ivo s. fins (1), heather davies (1), sean farrell (2), jose r.torres (3), gina pinchbeck (1), alan d. radford (1), peter-john noble (1) ((1) small animal veterinary surveillance network, institute of infection, veterinary and ecological sciences, university of liverpool, liverpool, uk, (2) department of computer science, durham university, durham, uk, (3) institute for animal health and food safety, university of las palmas de gran canaria, las palmas, canary archipelago, spain)","information retrieval","background: veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. here we compare the ability of a large language model (chatgpt) and a previously developed regular expression (regext) to identify overweight body condition scores (bcs) in veterinary narratives. methods: bcs values were extracted from 4,415 anonymised clinical narratives using either regext or by appending the narrative to a prompt sent to chatgpt coercing the model to return the bcs information. data were manually reviewed for comparison. results: the precision of regext was higher (100%, 95% ci 94.81-100%) than the chatgpt (89.3%; 95% ci82.75-93.64%). however, the recall of chatgpt (100%. 95% ci 96.18-100%) was considerably higher than that of regext (72.6%, 95% ci 63.92-79.94%). limitations: subtle prompt engineering is needed to improve chatgpt output. conclusions: large language models create diverse opportunities and, whilst complex, present an intuitive interface to information but require careful implementation to avoid unpredictable errors.",2023-08-03
"a new mapping of technological interdependence","a. fronzetti colladon, b. guardabascio, f. venturini","econometrics","how does technological interdependence affect a sector's ability to innovate? this paper answers this question by looking at knowledge interdependence (knowledge spillovers and technological complementarities) and structural interdependence (intersectoral network linkages). we examine these two dimensions of technological interdependence by applying novel methods of text mining and network analysis to the documents of 6.5 million patents granted by the united states patent and trademark office (uspto) between 1976 and 2021. we show that both dimensions positively affect sector innovation. while the impact of knowledge interdependence is slightly larger in the long-term horizon, positive shocks affecting the network linkages (structural interdependence) produce greater and more enduring effects on innovation performance in a relatively short run. our analysis also highlights that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations.",2023-07-31
"unscientify: detecting scientific uncertainty in scholarly full text","panggih kusuma ningrum, philipp mayr, iana atanassova","computation and language","this demo paper presents unscientify, an interactive system designed to detect scientific uncertainty in scholarly full text. the system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. the pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. additionally, unscientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.",2023-07-26
"exploring acceptance of autonomous vehicle policies using keybert and sna: targeting engineering students","jinwoo ha, dongsoo kim","social and information networks","this study aims to explore user acceptance of autonomous vehicle (av) policies with improved text-mining methods. recently, south korean policymakers have viewed autonomous driving car (adc) and autonomous driving robot (adr) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. they support the construction of v2i and v2v communication infrastructures for adc and recognize that adr is equivalent to pedestrians to promote its deployment into sidewalks. to fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of industrial, mechanical, and electronics-electrical-computer. one is the co-occurrence network analysis (cna) based on tf-iwf and dice coefficient, and the other is the contextual semantic network analysis (c-sna) based on both keybert, which extracts keywords that contextually represent the comments, and double cosine similarity. the reason for comparing these approaches is to balance interest not only in the implications for the av policies but also in the need to apply quality text mining to this research domain. significantly, the limitation of frequency-based text mining, which does not reflect textual context, and the trade-off of adjusting thresholds in semantic network analysis (sna) were considered. as the results of comparing the two approaches, the c-sna provided the information necessary to understand users' voices using fewer nodes and features than the cna. the users who pre-emptively understood the av policies based on their engineering literacy and the given texts revealed potential risks of the av accident policies. this study adds suggestions to manage these risks to support the successful deployment of avs on public roads.",2023-07-18
"the building data genome directory -- an open, comprehensive data sharing platform for building performance research","xiaoyu jin, chun fu, hussain kazmi, atilla balint, ada canaydin, matias quintana, filip biljecki, fu xiao, clayton miller","applications","the building sector plays a crucial role in the worldwide decarbonization effort, accounting for significant portions of energy consumption and environmental effects. however, the scarcity of open data sources is a continuous challenge for built environment researchers and practitioners. although several efforts have been made to consolidate existing open datasets, no database currently offers a comprehensive collection of building data types with all subcategories and time granularities (e.g., year, month, and sub-hour). this paper presents the building data genome directory, an open data-sharing platform serving as a one-stop shop for the data necessary for vital categories of building energy research. the data directory is an online portal (this http url) that allows filtering and discovering valuable datasets. the directory covers meter, building-level, and aggregated community-level data at the spatial scale and year-to-minute level at the temporal scale. the datasets were consolidated from a comprehensive exploration of sources, including governments, research institutes, and online energy dashboards. the results of this effort include the aggregation of 60 datasets pertaining to building energy ontologies, building energy models, building energy and water data, electric vehicle data, weather data, building information data, text-mining-based research data, image data of buildings, fault detection diagnosis data and occupant data. a crowdsourcing mechanism in the platform allows users to submit datasets they suggest for inclusion by filling out an online form. this directory can fuel research and applications on building energy efficiency, which is an essential step toward addressing the world's energy and environmental challenges.",2023-07-03
"information extraction in domain and generic documents: findings from heuristic-based and data-driven approaches","shiyu yuan, carlo lipizzi","computation and language","information extraction (ie) plays very important role in natural language processing (nlp) and is fundamental to many nlp applications that used to extract structured information from unstructured text data. heuristic-based searching and data-driven learning are two main stream implementation approaches. however, no much attention has been paid to document genre and length influence on ie tasks. to fill the gap, in this study, we investigated the accuracy and generalization abilities of heuristic-based searching and data-driven to perform two ie tasks: named entity recognition (ner) and semantic role labeling (srl) on domain-specific and generic documents with different length. we posited two hypotheses: first, short documents may yield better accuracy results compared to long documents; second, generic documents may exhibit superior extraction outcomes relative to domain-dependent documents due to training document genre limitations. our findings reveals that no single method demonstrated overwhelming performance in both tasks. for named entity extraction, data-driven approaches outperformed symbolic methods in terms of accuracy, particularly in short texts. in the case of semantic roles extraction, we observed that heuristic-based searching method and data-driven based model with syntax representation surpassed the performance of pure data-driven approach which only consider semantic information. additionally, we discovered that different semantic roles exhibited varying accuracy levels with the same method. this study offers valuable insights for downstream text mining tasks, such as ner and srl, when addressing various document features and genres.",2023-06-30
"dmner: biomedical entity recognition by detection and matching","junyi bian, rongze jiang, weiqi zhai, tianyang huang, hong zhou, shanfeng zhu","computation and language","biomedical named entity recognition (bner) serves as the foundation for numerous biomedical text mining tasks. unlike general ner, bner require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. in this study, we propose a novel bner framework called dmner. by leveraging existing entity representation models sapbert, we tackle bner as a two-step process: entity boundary detection and biomedical entity matching. dmner exhibits applicability across multiple ner scenarios: 1) in supervised ner, we observe that dmner effectively rectifies the output of baseline ner models, thereby further enhancing performance. 2) in distantly supervised ner, combining mrc and autoner as span boundary detectors enables dmner to achieve satisfactory results. 3) for training ner by merging multiple datasets, we adopt a framework similar to ds-ner but additionally leverage chatgpt to obtain high-quality phrases in the training. through extensive experiments conducted on 10 benchmark datasets, we demonstrate the versatility and effectiveness of dmner.",2023-06-27
"chatgpt chemistry assistant for text mining and prediction of mof synthesis","zhiling zheng, oufan zhang, christian borgs, jennifer t. chayes, omar m. yaghi","information retrieval","we use prompt engineering to guide chatgpt in the automation of text mining of metal-organic frameworks (mofs) synthesis conditions from diverse formats and styles of the scientific literature. this effectively mitigates chatgpt's tendency to hallucinate information -- an issue that previously made the use of large language models (llms) in scientific fields challenging. our approach involves the development of a workflow implementing three different processes for text mining, programmed by chatgpt itself. all of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. we deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 mofs sourced from peer-reviewed research articles. this process incorporates our chemprompt engineering strategy to instruct chatgpt in text mining, resulting in impressive precision, recall, and f1 scores of 90-99%. furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting mof experimental crystallization outcomes and preliminarily identifying important factors in mof crystallization. we also developed a reliable data-grounded mof chatbot to answer questions on chemical reactions and synthesis procedures. given that the process of using chatgpt reliably mines and tabulates diverse mof synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our chatgpt chemistry assistant will be very useful across various other chemistry sub-disciplines.",2023-06-20
"persian semantic role labeling using transfer learning and bert-based models","saeideh niksirat aghdam, sayyed ali hossayni, erfan khedersolh sadeh, nasim khozouei, behrouz minaei bidgoli","computation and language","semantic role labeling (srl) is the process of detecting the predicate-argument structure of each predicate in a sentence. srl plays a crucial role as a pre-processing step in many nlp applications such as topic and concept extraction, question answering, summarization, machine translation, sentiment analysis, and text mining. recently, in many languages, unified srl dragged lots of attention due to its outstanding performance, which is the result of overcoming the error propagation problem. however, regarding the persian language, all previous works have focused on traditional methods of srl leading to a drop in accuracy and imposing expensive feature extraction steps in terms of financial resources, time and energy consumption. in this work, we present an end-to-end srl method that not only eliminates the need for feature extraction but also outperforms existing methods in facing new samples in practical situations. the proposed method does not employ any auxiliary features and shows more than 16 (83.16) percent improvement in accuracy against previous methods in similar circumstances.",2023-06-17
"metricprompt: prompting model as a relevance metric for few-shot text classification","hongyuan dong, weinan zhang, wanxiang che","computation and language","prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. in this work, we propose metricprompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. metricprompt adopts prompting model as the relevance metric, further bridging the gap between pre-trained language model's (plm) pre-training objective and text classification task, making possible plm's smooth adaption. taking a training sample and a query one simultaneously, metricprompt captures cross-sample relevance information for accurate relevance estimation. we conduct experiments on three widely used text classification datasets across four few-shot settings. results show that metricprompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (sota) performance.",2023-06-15
"curatr: a platform for semantic analysis and curation of historical literary texts","susan leavy, gerardine meaney, karen wade, derek greene","computation and language","the increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. the scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. this paper presents curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. the platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.",2023-06-13
"recurrent attention networks for long-text modeling","xianming li, zongxi li, xiaotian luo, haoran xie, xing lee, yingbin zhao, fu lee wang, qing li","computation and language","self-attention-based models have achieved remarkable progress in short-text mining. however, the quadratic computational complexities restrict their application in long text processing. prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, recurrent attention network (ran), to enable the recurrent operation of self-attention. combining the advantages from both sides, the well-designed ran is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. furthermore, ran is computationally scalable as it supports parallelization on long document processing. extensive experiments demonstrate the long-text encoding ability of the proposed ran model on both classification and sequential tasks, showing its potential for a wide range of applications.",2023-06-12
"advancing italian biomedical information extraction with transformers-based models: methodological insights and multicenter practical application","claudio crema, tommaso mario buonocore, silvia fostinelli, enea parimbelli, federico verde, cira fundar√≤, marina manera, matteo cotta ramusino, marco capelli, alfredo costa, giuliano binetti, riccardo bellazzi, alberto redolfi","computation and language","the introduction of computerized medical records in hospitals has reduced burdensome activities like manual writing and information fetching. however, the data contained in medical records are still far underutilized, primarily because extracting data from unstructured textual medical records takes time and effort. information extraction, a subfield of natural language processing, can help clinical practitioners overcome this limitation by using automated text-mining pipelines. in this work, we created the first italian neuropsychiatric named entity recognition dataset, psynit, and used it to develop a transformers-based model. moreover, we collected and leveraged three external independent datasets to implement an effective multicenter model, with overall f1-score 84.77%, precision 83.16%, recall 86.44%. the lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a ""low-resource"" approach. this allowed us to establish methodological guidelines that pave the way for natural language processing studies in less-resourced languages.",2023-06-08
"analysis of the fed's communication by using textual entailment model of zero-shot classification","yasuhiro nakayama, tomochika sawaki","computation and language","in this study, we analyze documents published by central banks using text mining techniques and propose a method to evaluate the policy tone of central banks. since the monetary policies of major central banks have a broad impact on financial market trends, the pricing of risky assets, and the real economy, market participants are attempting to more accurately capture changes in the outlook for central banks' future monetary policies. since the published documents are also an important tool for the central bank to communicate with the market, they are meticulously elaborated on grammatical syntax and wording, and investors are urged to read more accurately about the central bank's policy stance. sentiment analysis on central bank documents has long been carried out, but it has been difficult to interpret the meaning of the documents accurately and to explicitly capture even the intentional change in nuance. this study attempts to evaluate the implication of the zero-shot text classification method for an unknown economic environment using the same model. we compare the tone of the statements, minutes, press conference transcripts of fomc meetings, and the fed officials' (chair, vice chair, and governors) speeches. in addition, the minutes of the fomc meetings were subjected to a phase analysis of changes in each policy stance since 1971.",2023-06-07
"patient dropout prediction in virtual health: a multimodal dynamic knowledge graph and text mining approach","shuang geng, wenli zhang, jiaheng xie, gemin liang, ben niu","machine learning","virtual health has been acclaimed as a transformative force in healthcare delivery. yet, its dropout issue is critical that leads to poor health outcomes, increased health, societal, and economic costs. timely prediction of patient dropout enables stakeholders to take proactive steps to address patients' concerns, potentially improving retention rates. in virtual health, the information asymmetries inherent in its delivery format, between different stakeholders, and across different healthcare delivery systems hinder the performance of existing predictive methods. to resolve those information asymmetries, we propose a multimodal dynamic knowledge-driven dropout prediction (mdkdp) framework that learns implicit and explicit knowledge from doctor-patient dialogues and the dynamic and complex networks of various stakeholders in both online and offline healthcare delivery systems. we evaluate mdkdp by partnering with one of the largest virtual health platforms in china. mdkdp improves the f1-score by 3.26 percentage points relative to the best benchmark. comprehensive robustness analyses show that integrating stakeholder attributes, knowledge dynamics, and compact bilinear pooling significantly improves the performance. our work provides significant implications for healthcare it by revealing the value of mining relations and knowledge across different service modalities. practically, mdkdp offers a novel design artifact for virtual health platforms in patient dropout management.",2023-06-06
"explaining hate speech classification with model agnostic methods","durgesh nandini, ute schmid","computation and language","there have been remarkable breakthroughs in machine learning and artificial intelligence, notably in the areas of natural language processing and deep learning. additionally, hate speech detection in dialogues has been gaining popularity among natural language processing researchers with the increased use of social media. however, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in ai models has been deeply realised. taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. this has been achieved by first predicting the classification of a text and then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. the bidirectional transformer model bert has been used for prediction because of its state of the art efficiency over other machine learning models. the model agnostic algorithm lime generates explanations for the output of a trained classifier and predicts the features that influence the model decision. the predictions generated from the model were evaluated manually, and after thorough evaluation, we observed that the model performs efficiently in predicting and explaining its prediction. lastly, we suggest further directions for the expansion of the provided research work.",2023-05-30
"mapping chatgpt in mainstream media to unravel jobs and diversity challenges: early quantitative insights through sentiment analysis and word frequency analysis","maya karanouh","computers and society","the exponential growth in user acquisition and popularity of openais chatgpt, an artificial intelligence(ai) powered chatbot, was accompanied by widespread mainstream media coverage. this article presents a quantitative data analysis of the early trends and sentiments revealed by conducting text mining and nlp methods onto a corpus of 10,902 mainstream news headlines related to the subject of chatgpt and artificial intelligence, from the launch of chatgpt in november 2022 to march 2023. the findings revealed in sentiment analysis, chatgpt and artificial intelligence, were perceived more positively than negatively in the mainstream media. in regards to word frequency results, over sixty-five percent of the top frequency words were focused on big tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. this article is a critical analysis into the power structures and collusions between big tech and big media in their hegemonic exclusion of diversity and job challenges from mainstream media.",2023-05-25
"the grammar and syntax based corpus analysis tool for the ukrainian language","daria stetsenko, inez okulska","computation and language","this paper provides an overview of a text mining tool the stylometrix developed initially for the polish language and further extended for english and recently for ukrainian. the stylometrix is built upon various metrics crafted manually by computational linguists and researchers from literary studies to analyze grammatical, stylistic, and syntactic patterns. the idea of constructing the statistical evaluation of syntactic and grammar features is straightforward and familiar for the languages like english, spanish, german, and others; it is yet to be developed for low-resource languages like ukrainian. we describe the stylometrix pipeline and provide some experiments with this tool for the text classification task. we also describe our package's main limitations and the metrics' evaluation procedure.",2023-05-22
"disco: distilled student models co-training for semi-supervised text mining","weifeng jiang, qianren mao, chenghua lin, jianxin li, ting deng, weiyi yang, zheng wang","computation and language","many text mining models are constructed by fine-tuning a large deep pre-trained language model (plm) in downstream tasks. however, a significant challenge nowadays is maintaining performance when we use a lightweight model with limited labelled samples. we present disco, a semi-supervised learning (ssl) framework for fine-tuning a cohort of small student models generated from a large plm using knowledge distillation. our key insight is to share complementary knowledge among distilled student cohorts to promote their ssl effectiveness. disco employs a novel co-training technique to optimize a cohort of multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. we evaluate disco on both semi-supervised text classification and extractive summarization tasks. experimental results show that disco can produce student models that are 7.6 times smaller and 4.8 times faster in inference than the baseline plms while maintaining comparable performance. we also show that disco-generated student models outperform the similar-sized models elaborately tuned in distinct tasks.",2023-05-20
"comparing variation in tokenizer outputs using a series of problematic and challenging biomedical sentences","christopher meaney, therese a stukel, peter c austin, michael escobar","computation and language","background & objective: biomedical text data are increasingly available for research. tokenization is an initial step in many biomedical text mining pipelines. tokenization is the process of parsing an input biomedical sentence (represented as a digital character sequence) into a discrete set of word/token symbols, which convey focused semantic/syntactic meaning. the objective of this study is to explore variation in tokenizer outputs when applied across a series of challenging biomedical sentences. method: diaz [2015] introduce 24 challenging example biomedical sentences for comparing tokenizer performance. in this study, we descriptively explore variation in outputs of eight tokenizers applied to each example biomedical sentence. the tokenizers compared in this study are the nltk white space tokenizer, the nltk penn tree bank tokenizer, spacy and scispacy tokenizers, stanza/stanza-craft tokenizers, the udpipe tokenizer, and r-tokenizers. results: for many examples, tokenizers performed similarly effectively; however, for certain examples, there were meaningful variation in returned outputs. the white space tokenizer often performed differently than other tokenizers. we observed performance similarities for tokenizers implementing rule-based systems (e.g. pattern matching and regular expressions) and tokenizers implementing neural architectures for token classification. oftentimes, the challenging tokens resulting in the greatest variation in outputs, are those words which convey substantive and focused biomedical/clinical meaning (e.g. x-ray, il-10, tcr/cd3, cd4+ cd8+, and (ca2+)-regulated). conclusion: when state-of-the-art, open-source tokenizers from python and r were applied to a series of challenging biomedical example sentences, we observed subtle variation in the returned outputs.",2023-05-15
"learner-centered analysis in educational metaverse environments: exploring value exchange systems through natural interaction and text mining","yun-cheng tsai","computers and society","this paper explores the potential developments of self-directed learning in the metaverse in response to education 4.0 and the fourth industrial revolution. it highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. additionally, it focuses on exploring value exchange systems through natural interaction, text mining, and analysis. the metaverse concept extends beyond extended reality (xr) technologies, encompassing digital avatars and shared ecological value. the role of educators in exploring new technologies and leveraging text-mining techniques to enhance learning efficiency is emphasized. the metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. this paper sheds light on the metaverse's potential to create a learner-centered educational environment and adapt to the evolving landscape of education 4.0. its findings, supported by text mining analysis, contribute to understanding the metaverse's role in shaping education in the fourth industrial revolution.",2023-05-15
"selfdocseg: a self-supervised vision-based approach towards document segmentation","subhajit maity, sanket biswas, siladittya manna, ayan banerjee, josep llad√≥s, saumik bhattacharya, umapada pal","computer vision and pattern recognition","document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. however, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. with growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. we address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tuning it with an object detection model. we show that our pipeline sets a new benchmark in this context and performs at par with the existing methods and the supervised counterparts, if not outperforms. the code is made publicly available at: this https url",2023-05-01
"patent mining by extracting functional analysis information modelled as graph structure: a patent knowledge-base collaborative building approach","manal e. helal, mohammed e. helal","databases","patents provide a rich source of information about design innovations. patent mining techniques employ various technologies, such as text mining, machine learning, natural language processing, and ontology-building techniques. an automated graph data modelling method is proposed for extracting functional representations for building a semantic database of patents of mechanical designs. the method has several benefits: the schema-free characteristic of the proposed graph modelling enables the ontology it is based on to evolve and generalise to upper ontologies across technology domains and to specify lower ontologies to more specific domains. graph modelling benefits from enhanced performance of deep queries across many levels of relationships and interactions and provides efficient storage. graph modelling also enables visualisation libraries to use the graph data structure immediately, avoiding the need for graph extraction programs from relational databases. patent/design comparisons are computed by search queries using counting of overlaps of different levels and weights. this work has produced the patmine solidworks add-in \c{opyright}, which compares annotated cad designs with patents and highlights overlapping design concepts. the patent annotation extracts its functional analysis, representing its structure as geometric feature interactions. additional features such as full-text search and semantic search of the patmine patents database are available, and graph analytic methods and machine learning algorithms are enabled and can be implemented as plug-ins in future work. keywords: patent mining; semantic analysis; functional analysis diagrams; graph data modelling; visualisation; similarity scoring; big data analytics; machine learning; artificial intelligence; natural language processing",2023-04-29
"assessing text mining and technical analyses on forecasting financial time series","ali lashgari","econometrics","forecasting financial time series (fts) is an essential field in finance and economics that anticipates market movements in financial markets. this paper investigates the accuracy of text mining and technical analyses in forecasting financial time series. it focuses on the s&p500 stock market index during the pandemic, which tracks the performance of the largest publicly traded companies in the us. the study compares two methods of forecasting the future price of the s&p500: text mining, which uses nlp techniques to extract meaningful insights from financial news, and technical analysis, which uses historical price and volume data to make predictions. the study examines the advantages and limitations of both methods and analyze their performance in predicting the s&p500. the finbert model outperforms other models in terms of s&p500 price prediction, as evidenced by its lower rmse value, and has the potential to revolutionize financial analysis and prediction using financial news data. keywords: arima, bert, finbert, forecasting financial time series, garch, lstm, technical analysis, text mining jel classifications: g4, c8",2023-04-27
"easyner: a customizable easy-to-use pipeline for deep learning- and dictionary-based named entity recognition from medical text","rafsan ahmed, petter berntsson, alexander skafte, salma kazemi rashed, marcus klang, adam barvesten, ola olde, william lindholm, antton lamarca arrizabalaga, pierre nugues, sonja aits","quantitative methods","background medical research generates millions of publications and it is a great challenge for researchers to utilize this information in full since its scale and complexity greatly surpasses human reading capabilities. automated text mining can help extract and connect information spread across this large body of literature but this technology is not easily accessible to life scientists. results here, we developed an easy-to-use end-to-end pipeline for deep learning- and dictionary-based named entity recognition (ner) of typical entities found in medical research articles, including diseases, cells, chemicals, genes/proteins, and species. the pipeline can access and process large medical research article collections (pubmed, cord-19) or raw text and incorporates a series of deep learning models fine-tuned on the huner corpora collection. in addition, the pipeline can perform dictionary-based ner related to covid-19 and other medical topics. users can also load their own ner models and dictionaries to include additional entities. the output consists of publication-ready ranked lists and graphs of detected entities and files containing the annotated texts. an associated script allows rapid inspection of the results for specific entities of interest. as model use cases, the pipeline was deployed on two collections of autophagy-related abstracts from pubmed and on the cord19 dataset, a collection of 764 398 research article abstracts related to covid-19. conclusions the ner pipeline we present is applicable in a variety of medical research settings and makes customizable text mining accessible to life scientists.",2023-04-16
"zero-shot multi-label topic inference with sentence encoders","souvika sarkar, dongji feng, shubhra kanti karmaker santu","computation and language","sentence encoders have indeed been shown to achieve superior performances for many downstream text-mining tasks and, thus, claimed to be fairly general. inspired by this, we performed a detailed study on how to leverage these sentence encoders for the ""zero-shot topic inference"" task, where the topics are defined/provided by the users in real-time. extensive experiments on seven different datasets demonstrate that sentence-bert demonstrates superior generality compared to other encoders, while universal sentence encoder can be preferred when efficiency is a top priority.",2023-04-14
"frenchmedmcqa: a french multiple-choice question answering dataset for medical domain","yanis labrak, adrien bazoge, richard dufour, mickael rouvier, emmanuel morin, b√©atrice daille, pierre-antoine gourraud","computation and language","this paper introduces frenchmedmcqa, the first publicly available multiple-choice question answering (mcqa) dataset in french for medical domain. it is composed of 3,105 questions taken from real exams of the french medical specialization diploma in pharmacy, mixing single and multiple answers. each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). we also propose first baseline models to automatically process this mcqa task in order to report on the current performances and to highlight the difficulty of the task. a detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the mcqa task: in our case, english specialized models yielded better results than generic french ones, even though frenchmedmcqa is in french. corpus, models and tools are available online.",2023-04-09
"multidimensional perceptron for efficient and explainable long text classification","yexiang wang, yating zhang, xiaozhong liu, changlong sun","computation and language","because of the inevitable cost and complexity of transformer and pre-trained models, efficiency concerns are raised for long text classification. meanwhile, in the highly sensitive domains, e.g., healthcare and legal long-text mining, potential model distrust, yet underrated and underexplored, may hatch vital apprehension. existing methods generally segment the long text, encode each piece with the pre-trained model, and use attention or rnns to obtain long text representation for classification. in this work, we propose a simple but effective model, segment-aware multidimensional perceptron (swipe), to replace attention/rnns in the above framework. unlike prior efforts, swipe can effectively learn the label of the entire text with supervised training, while perceive the labels of the segments and estimate their contributions to the long-text labeling in an unsupervised manner. as a general classifier, swipe can endorse different encoders, and it outperforms sota models in terms of classification accuracy and model efficiency. it is noteworthy that swipe achieves superior interpretability to transparentize long text classification results.",2023-04-04
"thematic context vector association based on event uncertainty for twitter","vaibhav khatavkar, swapnil mane, parag kulkarni","computation and language","keyword extraction is a crucial process in text mining. the extraction of keywords with respective contextual events in twitter data is a big challenge. the challenging issues are mainly because of the informality in the language used. the use of misspelled words, acronyms, and ambiguous terms causes informality. the extraction of keywords with informal language in current systems is pattern based or event based. in this paper, contextual keywords are extracted using thematic events with the help of data association. the thematic context for events is identified using the uncertainty principle in the proposed system. the thematic contexts are weighed with the help of vectors called thematic context vectors which signifies the event as certain or uncertain. the system is tested on the twitter covid-19 dataset and proves to be effective. the system extracts event-specific thematic context vectors from the test dataset and ranks them. the extracted thematic context vectors are used for the clustering of contextual thematic vectors which improves the silhouette coefficient by 0.5% than state of art methods namely tf and tf-idf. the thematic context vector can be used in other applications like cyberbullying, sarcasm detection, figurative language detection, etc.",2023-04-04
"informed machine learning, centrality, cnn, relevant document detection, repatriation of indigenous human remains","md abul bashar, richi nayak, gareth knapman, paul turnbull, cressida fforde","computation and language","among the pressing issues facing australian and other first nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in western scientific institutions. the success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. this article reports on collaborative research by data scientists and social science researchers in the research, reconcile, renew network (rrr) to develop and apply text mining techniques to identify this vital information. we describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. relevant/non-relevant) documents. to improve the accuracy of our detection model, we explore the use of an informed neural network (inn) model that describes documentary content using expert-informed contextual knowledge. only a few labelled documents are used to provide specificity to the model, using conceptually related keywords identified by rrr experts in provenance research. the results confirm the value of using an inn network model for identifying relevant documents related to the investigation of the global commercial trade in indigenous human remains. empirical analysis suggests that this inn model can be generalized for use by other researchers in the social sciences and humanities who want to extract relevant information from large textual corpora.",2023-03-25
"chatgpt as the transportation equity information source for scientific writing","boniphace kutela, shoujia li, subasish das, jinli liu","information retrieval","transportation equity is an interdisciplinary agenda that requires both transportation and social inputs. traditionally, transportation equity information are sources from public libraries, conferences, televisions, social media, among other. artificial intelligence (ai) tools including advanced language models such as chatgpt are becoming favorite information sources. however, their credibility has not been well explored. this study explored the content and usefulness of chatgpt-generated information related to transportation equity. it utilized 152 papers retrieved through the web of science (wos) repository. the prompt was crafted for chatgpt to provide an abstract given the title of the paper. the chatgpt-based abstracts were then compared to human-written abstracts using statistical tools and unsupervised text mining. the results indicate that a weak similarity between chatgpt and human-written abstracts. on average, the human-written abstracts and chatgpt generated abstracts were about 58% similar, with a maximum and minimum of 97% and 1.4%, respectively. the keywords from the abstracts of papers with over the mean similarity score were more likely to be similar whereas those from below the average score were less likely to be similar. themes with high similarity scores include access, public transit, and policy, among others. further, clear differences in the key pattern of clusters for high and low similarity score abstracts was observed. contrarily, the findings from collocated keywords were inconclusive. the study findings suggest that chatgpt has the potential to be a source of transportation equity information. however, currently, a great amount of attention is needed before a user can utilize materials from chatgpt",2023-03-10
"does synthetic data generation of llms help clinical text mining?","ruixiang tang, xiaotian han, xiaoqian jiang, xia hu","computation and language","recent advancements in large language models (llms) have led to the development of highly potent models like openai's chatgpt. these models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. however, their effectiveness in the healthcare sector remains uncertain. in this study, we seek to investigate the potential of chatgpt to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. however, our preliminary results indicate that employing chatgpt directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the chatgpt api. to overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing chatgpt and fine-tuning a local model for the downstream task. our method has resulted in significant improvements in the performance of downstream tasks, improving the f1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. furthermore, generating data using chatgpt can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. in summary, the proposed framework presents a promising solution to enhance the applicability of llm models to clinical text mining.",2023-03-08
"nlp workbench: efficient and extensible integration of state-of-the-art text mining tools","peiran yao, matej kosmajac, abeer waheed, kostyantyn guzhva, natalie hervieux, denilson barbosa","computation and language","nlp workbench is a web-based platform for text mining that allows non-expert users to obtain semantic understanding of large-scale corpora using state-of-the-art text mining models. the platform is built upon latest pre-trained models and open source systems from academia that provide semantic analysis functionalities, including but not limited to entity linking, sentiment analysis, semantic parsing, and relation extraction. its extensible design enables researchers and developers to smoothly replace an existing model or integrate a new one. to improve efficiency, we employ a microservice architecture that facilitates allocation of acceleration hardware and parallelization of computation. this paper presents the architecture of nlp workbench and discusses the challenges we faced in designing it. we also discuss diverse use cases of nlp workbench and the benefits of using it over other approaches. the platform is under active development, with its source code released under the mit license. a website and a short video demonstrating our platform are also available.",2023-03-02
"a text mining analysis of data protection politics: the case of plenary sessions of the european parliament","jukka ruohonen","computers and society","data protection laws and policies have been studied extensively in recent years, but little is known about the parliamentary politics of data protection. this imitation applies even to the european union (eu) that has taken the global lead in data protection and privacy regulation. for patching this notable gap in existing research, this paper explores the data protection questions raised by the members of the european parliament (meps) in the parliament's plenary sessions and the answers given to these by the european commission. over a thousand of such questions and answers are covered in a period from 1995 to early 2023. given computational analysis based on text mining, the results indicate that (a) data protection has been actively debated in the parliament during the past twenty years. no noticeable longitudinal trends are present; the debates have been relatively constant. as could be expected, (b) the specific data protection laws in the eu have frequently been referenced in these debates, which (c) do not seem to align along conventional political dimensions such as the left-right axis. furthermore, (d) numerous distinct data protection topics have been debated by the parliamentarians, indicating that data protection politics in the eu go well-beyond the recently enacted regulations.",2023-02-20
"lightweight transformers for clinical natural language processing","omid rohanian, mohammadmahdi nouriborji, hannah jauncey, samaneh kouchaki, isaric clinical characterisation group, lei clifton, laura merson, david a. clifton","computation and language","specialised pre-trained language models are becoming more frequent in nlp since they can potentially outperform models trained on generic texts. biobert and bioclinicalbert are two examples of such models that have shown promise in medical nlp tasks. many of these models are overparametrised and resource-intensive, but thanks to techniques like knowledge distillation (kd), it is possible to create smaller versions that perform almost as well as their larger counterparts. in this work, we specifically focus on development of compact language models for processing clinical texts (i.e. progress notes, discharge summaries etc). we developed a number of efficient lightweight clinical transformers using knowledge distillation and continual learning, with the number of parameters ranging from 15 million to 65 million. these models performed comparably to larger models such as biobert and clinicalbiobert and significantly outperformed other compact models trained on general or biomedical data. our extensive evaluation was done across several standard datasets and covered a wide range of clinical text-mining tasks, including natural language inference, relation extraction, named entity recognition, and sequence classification. to our knowledge, this is the first comprehensive study specifically focused on creating efficient and compact transformers for clinical nlp tasks. the models and code used in this study can be found on our huggingface profile at this https url and github page at this https url, respectively, promoting reproducibility of our results.",2023-02-09
"real-word error correction with trigrams: correcting multiple errors in a sentence","seyed mohammadsadegh dashti","computation and language","spelling correction is a fundamental task in text mining. in this study, we assess the real-word error correction model proposed by mays, damerau and mercer and describe several drawbacks of the model. we propose a new variation which focuses on detecting and correcting multiple real-word errors in a sentence, by manipulating a probabilistic context-free grammar (pcfg) to discriminate between items in the search space. we test our approach on the wall street journal corpus and show that it outperforms hirst and budanitsky's wordnet-based method and wilcox-o'hearn, hirst, and budanitsky's fixed windows size method.-o'hearn, hirst, and budanitsky's fixed windows size method.",2023-02-07
"estimation of gaussian bi-clusters with general block-diagonal covariance matrix and applications","anastasiia livochka, ryan browne, sanjeena subedi","computation","bi-clustering is a technique that allows for the simultaneous clustering of observations and features in a dataset. this technique is often used in bioinformatics, text mining, and time series analysis. an important advantage of biclustering algorithm is the ability to uncover multiple ``views'' (i.e., through rows and column groupings) in the data. several gaussian mixture model based biclustering approach currently exist in the literature. however, they impose severe restrictions on the structure of the covariance matrix. here, we propose a gaussian mixture model-based bi-clustering approach that provides a more flexible block-diagonal covariance structure. we show that the clustering accuracy of the proposed model is comparable to other known techniques but our approach provides a more flexible covariance structure and has substantially lower computational time. we demonstrate the application of the proposed model in bioinformatics and topic modelling.",2023-02-08
"precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature","tanjin he, haoyan huo, christopher j. bartel, zheren wang, kevin cruse, gerbrand ceder","materials science","synthesis prediction is a key accelerator for the rapid design of advanced materials. however, determining synthesis variables such as the choice of precursor materials is challenging for inorganic materials because the sequence of reactions during heating is not well understood. in this work, we use a knowledge base of 29,900 solid-state synthesis recipes, text-mined from the scientific literature, to automatically learn which precursors to recommend for the synthesis of a novel target material. the data-driven approach learns chemical similarity of materials and refers the synthesis of a new target to precedent synthesis procedures of similar materials, mimicking human synthesis design. when proposing five precursor sets for each of 2,654 unseen test target materials, the recommendation strategy achieves a success rate of at least 82%. our approach captures decades of heuristic synthesis data in a mathematical form, making it accessible for use in recommendation engines and autonomous laboratories.",2023-02-05
"bioformer: an efficient transformer language model for biomedical text mining","li fang, qingyu chen, chih-hsuan wei, zhiyong lu, kai wang","computation and language","pretrained language models such as bidirectional encoder representations from transformers (bert) have achieved state-of-the-art performance in natural language processing (nlp) tasks. recently, bert has been adapted to the biomedical domain. despite the effectiveness, these models have hundreds of millions of parameters and are computationally expensive when applied to large-scale nlp applications. we hypothesized that the number of parameters of the original bert can be dramatically reduced with minor impact on performance. in this study, we present bioformer, a compact bert model for biomedical text mining. we pretrained two bioformer models (named bioformer8l and bioformer16l) which reduced the model size by 60% compared to bertbase. bioformer uses a biomedical vocabulary and was pre-trained from scratch on pubmed abstracts and pubmed central full-text articles. we thoroughly evaluated the performance of bioformer as well as existing biomedical bert models including biobert and pubmedbert on 15 benchmark datasets of four different biomedical nlp tasks: named entity recognition, relation extraction, question answering and document classification. the results show that with 60% fewer parameters, bioformer16l is only 0.1% less accurate than pubmedbert while bioformer8l is 0.9% less accurate than pubmedbert. both bioformer16l and bioformer8l outperformed biobertbase-v1.1. in addition, bioformer16l and bioformer8l are 2-3 fold as fast as pubmedbert/biobertbase-v1.1. bioformer has been successfully deployed to pubtator central providing gene annotations over 35 million pubmed abstracts and 5 million pubmed central full-text articles. we make bioformer publicly available via this https url, including pre-trained models, datasets, and instructions for downstream use.",2023-02-03
"curriculum-guided abstractive summarization for mental health online posts","sajad sotudeh, nazli goharian, hanieh deilamsalehy, franck dernoncourt","computation and language","automatically generating short summaries from users' online mental health posts could save counselors' reading time and reduce their fatigue so that they can provide timely responses to those seeking help for improving their mental state. recent transformers-based summarization models have presented a promising approach to abstractive summarization. they go beyond sentence selection and extractive strategies to deal with more complicated tasks such as novel word generation and sentence paraphrasing. nonetheless, these models have a prominent shortcoming; their training strategy is not quite efficient, which restricts the model's performance. in this paper, we include a curriculum learning approach to reweigh the training samples, bringing about an efficient learning procedure. we apply our model on extreme summarization dataset of mentsum posts -- a dataset of mental health related posts from reddit social media. compared to the state-of-the-art model, our proposed method makes substantial gains in terms of rouge and bertscore evaluation metrics, yielding 3.5% (rouge-1), 10.4% (rouge-2), and 4.7% (rouge-l), 1.5% (bertscore) relative improvements.",2023-02-02
"improving the inference of topic models via infinite latent state replications","daniel rugeles, zhen hai, juan felipe carmona, manoranjan dash, gao cong","computation and language","in text mining, topic models are a type of probabilistic generative models for inferring latent semantic topics from text corpus. one of the most popular inference approaches to topic models is perhaps collapsed gibbs sampling (cgs), which typically samples one single topic label for each observed document-word pair. in this paper, we aim at improving the inference of cgs for topic models. we propose to leverage state augmentation technique by maximizing the number of topic samples to infinity, and then develop a new inference approach, called infinite latent state replication (ilr), to generate robust soft topic assignment for each given document-word pair. experimental results on the publicly available datasets show that ilr outperforms cgs for inference of existing established topic models.",2023-01-25
"understanding finetuning for factual knowledge extraction from language models","mehran kazemi, sid mittal, deepak ramachandran","computation and language","language models (lms) pretrained on large corpora of text from the web have been observed to contain large amounts of various types of knowledge about the world. this observation has led to a new and exciting paradigm in knowledge graph construction where, instead of manual curation or text mining, one extracts knowledge from the parameters of an lm. recently, it has been shown that finetuning lms on a set of factual knowledge makes them produce better answers to queries from a different set, thus making finetuned lms a good candidate for knowledge extraction and, consequently, knowledge graph construction. in this paper, we analyze finetuned lms for factual knowledge extraction. we show that along with its previously known positive effects, finetuning also leads to a (potentially harmful) phenomenon which we call frequency shock, where at the test time the model over-predicts rare entities that appear in the training set and under-predicts common entities that do not appear in the training set enough times. we show that frequency shock leads to a degradation in the predictions of the model and beyond a point, the harm from frequency shock can even outweigh the positive effects of finetuning, making finetuning harmful overall. we then consider two solutions to remedy the identified negative effect: 1- model mixing and 2- mixture finetuning with the lm's pre-training task. the two solutions combined lead to significant improvements compared to vanilla finetuning.",2023-01-26
"cross-institution text mining to uncover clinical associations: a case study relating social factors and code status in intensive care medicine","madhumita sushil, atul j. butte, ewoud schuit, maarten van smeden, artuur m. leeuwenberg","computation and language","objective: text mining of clinical notes embedded in electronic medical records is increasingly used to extract patient characteristics otherwise not or only partly available, to assess their association with relevant health outcomes. as manual data labeling needed to develop text mining models is resource intensive, we investigated whether off-the-shelf text mining models developed at external institutions, together with limited within-institution labeled data, could be used to reliably extract study variables to conduct association studies. materials and methods: we developed multiple text mining models on different combinations of within-institution and external-institution data to extract social factors from discharge reports of intensive care patients. subsequently, we assessed the associations between social factors and having a do-not-resuscitate/intubate code. results: important differences were found between associations based on manually labeled data compared to text-mined social factors in three out of five cases. adopting external-institution text mining models using manually labeled within-institution data resulted in models with higher f1-scores, but not in meaningfully different associations. discussion: while text mining facilitated scaling analyses to larger samples leading to discovering a larger number of associations, the estimates may be unreliable. confirmation is needed with better text mining models, ideally on a larger manually labeled dataset. conclusion: the currently used text mining models were not sufficiently accurate to be used reliably in an association study. model adaptation using within-institution data did not improve the estimates. further research is needed to set conditions for reliable use of text mining in medical research.",2023-01-16
"mining healthcare procurement data using text mining and natural language processing -- reflection from an industrial project","ziqi zhang, tomas jasaitis, richard freeman, rowida alfrjani, adam funk","computation and language","while text mining and nlp research has been established for decades, there remain gaps in the literature that reports the use of these techniques in building real-world applications. for example, they typically look at single and sometimes simplified tasks, and do not discuss in-depth data heterogeneity and inconsistency that is common in real-world problems or their implication on the development of their methods. also, few prior work has focused on the healthcare domain. in this work, we describe an industry project that developed text mining and nlp solutions to mine millions of heterogeneous, multilingual procurement documents in the healthcare sector. we extract structured procurement contract data that is used to power a platform for dynamically assessing supplier risks. our work makes unique contributions in a number of ways. first, we deal with highly heterogeneous, multilingual data and we document our approach to tackle these challenges. this is mainly based on a method that effectively uses domain knowledge and generalises to multiple text mining and nlp tasks and languages. second, applying this method to mine millions of procurement documents, we develop the first structured procurement contract database that will help facilitate the tendering process. second, finally, we discuss lessons learned for practical text mining/nlp development, and make recommendations for future research and practice.",2023-01-09
"testing high-dimensional multinomials with applications to text analysis","t. tony cai, zheng tracy ke, paxton turner","methodology","motivated by applications in text mining and discrete distribution inference, we investigate the testing for equality of probability mass functions of $k$ groups of high-dimensional multinomial distributions. a test statistic, which is shown to have an asymptotic standard normal distribution under the null, is proposed. the optimal detection boundary is established, and the proposed test is shown to achieve this optimal detection boundary across the entire parameter space of interest. the proposed method is demonstrated in simulation studies and applied to analyze two real-world datasets to examine variation among consumer reviews of amazon movies and diversity of statistical paper abstracts.",2023-01-03
"understanding the main failure scenarios of subsea blowout preventers systems: an approach through latent semantic analysis","gustavo jorge martins de aguiar, ramon baptista narcizo, rodolfo cardoso, iara tammela, edwin benito mitacc meza, danilo colombo, luiz ant√¥nio de oliveira chaves, jamile eleut√©rio delesposte","information retrieval","the blowout preventer (bop) system is one of the most important well safety barriers during the drilling phase because it can prevent the development of blowout events. this paper investigates bop system's main failures using an lsa-based methodology. a total of 1312 failure records from companies worldwide were collected from the international association of drilling contractors' rapid-s53 database. the database contains recordings of halted drilling operations due to bop system's failures and component's function deviations. the main failure scenarios of the components annular preventer, shear rams preventer, compensated chamber solenoid valve, and hydraulic regulators were identified using the proposed methodology. the scenarios contained valuable information about corrective maintenance procedures, such as frequently observed failure modes, detection methods used, suspected causes, and corrective actions. the findings highlighted that the major failures of the components under consideration were leakages caused by damaged elastomeric seals. the majority of the failures were detected during function and pressure tests with the bop system in the rig. this study provides an alternative safety analysis that contributes to understanding blowout preventer system's critical component failures by applying a methodology based on a well-established text mining technique and analyzing failure records from an international database.",2023-01-02
"clustop: an unsupervised and integrated text clustering and topic extraction framework","zhongtao chen, chenghu mi, siwei duo, jingfei he, yatong zhou","computation and language","text clustering and topic extraction are two important tasks in text mining. usually, these two tasks are performed separately. for topic extraction to facilitate clustering, we can first project texts into a topic space and then perform a clustering algorithm to obtain clusters. to promote topic extraction by clustering, we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics. however, this naive strategy ignores the fact that text clustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship. performing them separately fails to make them mutually benefit each other to achieve the best overall performance. in this paper, we propose an unsupervised text clustering and topic extraction framework (clustop) which integrates text clustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously. our framework includes four components: enhanced language model training, dimensionality reduction, clustering and topic extraction, where the enhanced language model can be viewed as a bridge between clustering and topic extraction. on one hand, it provides text embeddings with a strong cluster structure which facilitates effective text clustering; on the other hand, it pays high attention on the topic related words for topic extraction because of its self-attention architecture. moreover, the training of enhanced language model is unsupervised. experiments on two datasets demonstrate the effectiveness of our framework and provide benchmarks for different model combinations in this framework.",2023-01-03
"irt2: inductive linking and ranking in knowledge graphs of varying scale","felix hamann, adrian ulges, maurice falk","machine learning","we address the challenge of building domain-specific knowledge models for industrial use cases, where labelled data and taxonomic information is initially scarce. our focus is on inductive link prediction models as a basis for practical tools that support knowledge engineers with exploring text collections and discovering and linking new (so-called open-world) entities to the knowledge graph. we argue that - though neural approaches to text mining have yielded impressive results in the past years - current benchmarks do not reflect the typical challenges encountered in the industrial wild properly. therefore, our first contribution is an open benchmark coined irt2 (inductive reasoning with text) that (1) covers knowledge graphs of varying sizes (including very small ones), (2) comes with incidental, low-quality text mentions, and (3) includes not only triple completion but also ranking, which is relevant for supporting experts with discovery tasks. we investigate two neural models for inductive link prediction, one based on end-to-end learning and one that learns from the knowledge graph and text data in separate steps. these models compete with a strong bag-of-words baseline. the results show a significant advance in performance for the neural approaches as soon as the available graph data decreases for linking. for ranking, the results are promising, and the neural approaches outperform the sparse retriever by a wide margin.",2023-01-02
"explainable ai for bioinformatics: methods, tools, and applications","md. rezaul karim, tanhim islam, oya beyan, christoph lange, michael cochez, dietrich rebholz-schuhmann, stefan decker","quantitative methods","artificial intelligence (ai) systems utilizing deep neural networks (dnns) and machine learning (ml) algorithms are widely used for solving important problems in bioinformatics, biomedical informatics, and precision medicine. however, complex dnns or ml models, which are often perceived as opaque and black-box, can make it difficult to understand the reasoning behind their decisions. this lack of transparency can be a challenge for both end-users and decision-makers, as well as ai developers. additionally, in sensitive areas like healthcare, explainability and accountability are not only desirable but also legally required for ai systems that can have a significant impact on human lives. fairness is another growing concern, as algorithmic decisions should not show bias or discrimination towards certain groups or individuals based on sensitive attributes. explainable artificial intelligence (xai) aims to overcome the opaqueness of black-box models and provide transparency in how ai systems make decisions. interpretable ml models can explain how they make predictions and the factors that influence their outcomes. however, most state-of-the-art interpretable ml methods are domain-agnostic and evolved from fields like computer vision, automated reasoning, or statistics, making direct application to bioinformatics problems challenging without customization and domain-specific adaptation. in this paper, we discuss the importance of explainability in the context of bioinformatics, provide an overview of model-specific and model-agnostic interpretable ml methods and tools, and outline their potential caveats and drawbacks. besides, we discuss how to customize existing interpretable ml methods for bioinformatics problems. nevertheless, we demonstrate how xai methods can improve transparency through case studies in bioimaging, cancer genomics, and text mining.",2022-12-25
"word embedding neural networks to advance knee osteoarthritis research","soheyla amirian, husam ghazaleh, mehdi assefi, hilal maradit kremers, hamid r. arabnia, johannes f. plate, ahmad p. tafti","artificial intelligence","osteoarthritis (oa) is the most prevalent chronic joint disease worldwide, where knee oa takes more than 80% of commonly affected joints. knee oa is not a curable disease yet, and it affects large columns of patients, making it costly to patients and healthcare systems. etiology, diagnosis, and treatment of knee oa might be argued by variability in its clinical and physical manifestations. although knee oa carries a list of well-known terminology aiming to standardize the nomenclature of the diagnosis, prognosis, treatment, and clinical outcomes of the chronic joint disease, in practice there is a wide range of terminology associated with knee oa across different data sources, including but not limited to biomedical literature, clinical notes, healthcare literacy, and health-related social media. among these data sources, the scientific articles published in the biomedical literature usually make a principled pipeline to study disease. rapid yet, accurate text mining on large-scale scientific literature may discover novel knowledge and terminology to better understand knee oa and to improve the quality of knee oa diagnosis, prevention, and treatment. the present works aim to utilize artificial neural network strategies to automatically extract vocabularies associated with knee oa diseases. our finding indicates the feasibility of developing word embedding neural networks for autonomous keyword extraction and abstraction of knee oa.",2022-12-22
"graph-based semantical extractive text analysis","mina samizadeh","computation and language","in the past few decades, there has been an explosion in the amount of available data produced from various sources with different topics. the availability of this enormous data necessitates us to adopt effective computational tools to explore the data. this leads to an intense growing interest in the research community to develop computational methods focused on processing this text data. a line of study focused on condensing the text so that we are able to get a higher level of understanding in a shorter time. the two important tasks to do this are keyword extraction and text summarization. in keyword extraction, we are interested in finding the key important words from a text. this makes us familiar with the general topic of a text. in text summarization, we are interested in producing a short-length text which includes important information about the document. the textrank algorithm, an unsupervised learning method that is an extension of the pagerank (algorithm which is the base algorithm of google search engine for searching pages and ranking them) has shown its efficacy in large-scale text mining, especially for text summarization and keyword extraction. this algorithm can automatically extract the important parts of a text (keywords or sentences) and declare them as the result. however, this algorithm neglects the semantic similarity between the different parts. in this work, we improved the results of the textrank algorithm by incorporating the semantic similarity between parts of the text. aside from keyword extraction and text summarization, we develop a topic clustering algorithm based on our framework which can be used individually or as a part of generating the summary to overcome coverage problems.",2022-12-19
"very large language model as a unified methodology of text mining","meng jiang","databases","text data mining is the process of deriving essential information from language text. typical text mining tasks include text categorization, text clustering, topic modeling, information extraction, and text summarization. various data sets are collected and various algorithms are designed for the different types of tasks. in this paper, i present a blue sky idea that very large language model (vllm) will become an effective unified methodology of text mining. i discuss at least three advantages of this new methodology against conventional methods. finally i discuss the challenges in the design and development of vllm techniques for text mining.",2022-12-19
"earthquake impact analysis based on text mining and social media analytics","zhe zheng, hong-zheng shi, yu-cheng zhou, xin-zheng lu, jia-rui lin","computation and language","earthquakes have a deep impact on wide areas, and emergency rescue operations may benefit from social media information about the scope and extent of the disaster. therefore, this work presents a text miningbased approach to collect and analyze social media data for early earthquake impact analysis. first, disasterrelated microblogs are collected from the sina microblog based on crawler technology. then, after data cleaning a series of analyses are conducted including (1) the hot words analysis, (2) the trend of the number of microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and rule-based text classification for earthquake impact analysis. finally, two recent earthquakes with the same magnitude and focal depth in china are analyzed to compare their impacts. the results show that the public opinion trend analysis and the trend of public opinion sentiment can estimate the earthquake's social impact at an early stage, which will be helpful to decision-making and rescue management.",2022-12-12
"evaluating airline service quality through the comprehensive text-mining and topsis-vikor-aism analysis","haotian xie, yi li, yang pu, chen zhang, junlin huang","applications","service quality rankings are pivotal for maintaining sustainability in the fiercely competitive airline industry. however, prior research in this domain has often fallen short in aspects of sample size, efficiency, and dependability. this study introduces refined insights into this area and establishes a comprehensive, yet highly elucidative, ranking framework. initially, we employ latent semantic analysis (lsa) to distill principal themes and sentiments from online reviews of 80 airlines. subsequently, we utilize the sentiwordnet lexicon and the textblob package for conducting sentiment analysis based on these reviews. following this, we construct a hierarchical structure using the computation of compromise solutions, employing an integrated technique for order preference by similarity to ideal solution, vis-√†-vis kriterijumska optimizacija i kompromisno resenje-adversarial interpretive structural model (topsis-vikor-aism) methodology. beyond aiding consumer decision-making and fostering airline growth, this study contributes novel viewpoints on evaluating the efficacy of airlines and other sectors.",2022-12-13
"text mining-based patent analysis for automated rule checking in aec","zhe zheng, bo-rui kang, qi-tian yuan, yu-cheng zhou, xin-zheng lu, jia-rui lin","information retrieval","automated rule checking (arc), which is expected to promote the efficiency of the compliance checking process in the architecture, engineering, and construction (aec) industry, is gaining increasing attention. throwing light on the arc application hotspots and forecasting its trends are useful to the related research and drive innovations. therefore, this study takes the patents from the database of the derwent innovations index database (dii) and china national knowledge infrastructure (cnki) as data sources and then carried out a three-step analysis including (1) quantitative characteristics (i.e., annual distribution analysis) of patents, (2) identification of arc topics using a latent dirichlet allocation (lda) and, (3) sna-based co-occurrence analysis of arc topics. the results show that the research hotspots and trends of chinese and english patents are different. the contributions of this study have three aspects: (1) an approach to a comprehensive analysis of patents by integrating multiple text mining methods (i.e., sna and lda) is introduced ; (2) the application hotspots and development trends of arc are reviewed based on patent analysis; and (3) a signpost for technological development and innovation of arc is provided.",2022-12-12
"robust convex biclustering with a tuning-free method","yifan chen, chunyin lei, chuanquan li, haiqiang ma, ningyuan hu","methodology","biclustering is widely used in different kinds of fields including gene information analysis, text mining, and recommendation system by effectively discovering the local correlation between samples and features. however, many biclustering algorithms will collapse when facing heavy-tailed data. in this paper, we propose a robust version of convex biclustering algorithm with huber loss. yet, the newly introduced robustification parameter brings an extra burden to selecting the optimal parameters. therefore, we propose a tuning-free method for automatically selecting the optimal robustification parameter with high efficiency. the simulation study demonstrates the more fabulous performance of our proposed method than traditional biclustering methods when encountering heavy-tailed noise. a real-life biomedical application is also presented. the r package rcvxbiclustr is available at this https url.",2022-12-06
"aioner: all-in-one scheme-based biomedical named entity recognition using deep learning","ling luo, chih-hsuan wei, po-ting lai, robert leaman, qingyu chen, zhiyong lu","computation and language","biomedical named entity recognition (bioner) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. manually labeling training data for the bioner task is costly, however, due to the significant domain expertise required for accurate annotation. the resulting data scarcity causes current bioner approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). we therefore propose a novel all-in-one (aio) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of bioner models. we further present aioner, a general-purpose bioner tool based on cutting-edge deep learning and our aio schema. we evaluate aioner on 14 bioner benchmark tasks and show that aioner is effective, robust, and compares favorably to other state-of-the-art approaches such as multi-task learning. we further demonstrate the practical utility of aioner in three independent tasks to recognize entity types not previously seen in training data, as well as the advantages of aioner over existing methods for processing biomedical text at a large scale (e.g., the entire pubmed data).",2022-11-30
"gapped string indexing in subquadratic space and sublinear query time","philip bille, inge li g√∏rtz, moshe lewenstein, solon p. pissis, eva rotenberg, teresa anna steiner","data structures and algorithms","in gapped string indexing, the goal is to compactly represent a string $s$ of length $n$ such that for any query consisting of two strings $p_1$ and $p_2$, called patterns, and an integer interval $[\alpha, \beta]$, called gap range, we can quickly find occurrences of $p_1$ and $p_2$ in $s$ with distance in $[\alpha, \beta]$. gapped string indexing is a central problem in computational biology and text mining and has thus received significant research interest, including parameterized and heuristic approaches. despite this interest, the best-known time-space trade-offs for gapped string indexing are the straightforward $o(n)$ space and $o(n+occ)$ query time or $\omega(n^2)$ space and $\tilde{o}(|p_1| + |p_2| + occ)$ query time. we break through this barrier obtaining the first interesting trade-offs with polynomially subquadratic space and polynomially sublinear query time. in particular, we show that, for every $0\leq \delta \leq 1$, there is a data structure for gapped string indexing with either $\tilde{o}(n^{2-\delta/3})$ or $\tilde{o}(n^{3-2\delta})$ space and $\tilde{o}(|p_1| + |p_2| + n^{\delta}\cdot (occ+1))$ query time, where $occ$ is the number of reported occurrences. as a new tool towards obtaining our main result, we introduce the shifted set intersection problem. we show that this problem is equivalent to the indexing variant of 3sum (3sum indexing). via a series of reductions, we obtain a solution to the gapped string indexing problem. furthermore, we enhance our data structure for deciding shifted set intersection, so that we can support the reporting variant of the problem. via the obtained equivalence to 3sum indexing, we thus give new improved data structures for the reporting variant of 3sum indexing, and we show how this improves upon the state-of-the-art solution for jumbled indexing for any alphabet of constant size $\sigma>5$.",2022-11-30
"a survey of relevant text mining technology","claudia peersman, matthew edwards, emma williams, awais rashid","cryptography and security","recent advances in text mining and natural language processing technology have enabled researchers to detect an authors identity or demographic characteristics, such as age and gender, in several text genres by automatically analysing the variation of linguistic characteristics. however, applying such techniques in the wild, i.e., in both cybercriminal and regular online social media, differs from more general applications in that its defining characteristics are both domain and process dependent. this gives rise to a number of challenges of which contemporary research has only scratched the surface. more specifically, a text mining approach applied on social media communications typically has no control over the dataset size, the number of available communications will vary across users. hence, the system has to be robust towards limited data availability. additionally, the quality of the data cannot be guaranteed. as a result, the approach needs to be tolerant to a certain degree of linguistic noise (for example, abbreviations, non-standard language use, spelling variations and errors). finally, in the context of cybercriminal fora, it has to be robust towards deceptive or adversarial behaviour, i.e. offenders who attempt to hide their criminal intentions (obfuscation) or who assume a false digital persona (imitation), potentially using coded language. in this work we present a comprehensive survey that discusses the problems that have already been addressed in current literature and review potential solutions. additionally, we highlight which areas need to be given more attention.",2022-11-28
"ai knows which words will appear in next year's korean csat","byunghyun ban, jejong lee, hyeonmok hwang","computation and language","a text-mining-based word class categorization method and lstm-based vocabulary pattern prediction method are introduced in this paper. a preprocessing method based on simple text appearance frequency analysis is first described. this method was developed as a data screening tool but showed 4.35 ~ 6.21 times higher than previous works. an lstm deep learning method is also suggested for vocabulary appearance pattern prediction method. ai performs a regression with various size of data window of previous exams to predict the probabilities of word appearance in the next exam. predicted values of ai over various data windows are processed into a single score as a weighted sum, which we call an ""ai-score"", which represents the probability of word appearance in next year's exam. suggested method showed 100% accuracy at the range 100-score area and showed only 1.7% error of prediction in the section where the scores were over 60 points. all source codes are freely available at the authors' git hub repository. (this https url)",2022-11-24
"automating systematic literature reviews with natural language processing and text mining: a systematic literature review","girish sundaram, daniel berleant","information retrieval","objectives: an slr is presented focusing on text mining based automation of slr creation. the present review identifies the objectives of the automation studies and the aspects of those steps that were automated. in so doing, the various ml techniques used, challenges, limitations and scope of further research are explained. methods: accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of slr. twenty-nine studies were analyzed. results: this review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ml techniques used, challenges, limitations and scope of further research. discussion: we describe uses of nlp/tm techniques to support increased automation of systematic literature reviews. this area has attracted increase attention in the last decade due to significant gaps in the applicability of tm to automate steps in the slr process. there are significant gaps in the application of tm and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. there is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews.",2022-11-20
"method for determining the similarity of text documents for the kazakh language, taking into account synonyms: extension to tf-idf","bakhyt bakiyev","information retrieval","the task of determining the similarity of text documents has received considerable attention in many areas such as information retrieval, text mining, natural language processing (nlp) and computational linguistics. transferring data to numeric vectors is a complex task where algorithms such as tokenization, stopword filtering, stemming, and weighting of terms are used. the term frequency - inverse document frequency (tf-idf) is the most widely used term weighting method to facilitate the search for relevant documents. to improve the weighting of terms, a large number of tf-idf extensions are made. in this paper, another extension of the tf-idf method is proposed where synonyms are taken into account. the effectiveness of the method is confirmed by experiments on functions such as cosine, dice and jaccard to measure the similarity of text documents for the kazakh language.",2022-11-22
"a large-scale dataset for biomedical keyphrase generation","mael houbre, florian boudin, beatrice daille","computation and language","keyphrase generation is the task consisting in generating a set of words or phrases that highlight the main topics of a document. there are few datasets for keyphrase generation in the biomedical domain and they do not meet the expectations in terms of size for training generative models. in this paper, we introduce kp-biomed, the first large-scale biomedical keyphrase generation dataset with more than 5m documents collected from pubmed abstracts. we train and release several generative models and conduct a series of experiments showing that using large scale datasets improves significantly the performances for present and absent keyphrase generation. the dataset is available under cc-by-nc v4.0 license at this https url datasets/taln-ls2n/kpbiomed.",2022-11-22
"explainable model-specific algorithm selection for multi-label classification","ana kostovska, carola doerr, sa≈°o d≈æeroski, dragi kocev, panƒçe panov, tome eftimov","machine learning","multi-label classification (mlc) is an ml task of predictive modeling in which a data instance can simultaneously belong to multiple classes. mlc is increasingly gaining interest in different application domains such as text mining, computer vision, and bioinformatics. several mlc algorithms have been proposed in the literature, resulting in a meta-optimization problem that the user needs to address: which mlc approach to select for a given dataset? to address this algorithm selection problem, we investigate in this work the quality of an automated approach that uses characteristics of the datasets - so-called features - and a trained algorithm selector to choose which algorithm to apply for a given task. for our empirical evaluation, we use a portfolio of 38 datasets. we consider eight mlc algorithms, whose quality we evaluate using six different performance metrics. we show that our automated algorithm selector outperforms any of the single mlc algorithms, and this is for all evaluated performance measures. our selection approach is explainable, a characteristic that we exploit to investigate which meta-features have the largest influence on the decisions made by the algorithm selector. finally, we also quantify the importance of the most significant meta-features for various domains.",2022-11-21
"coronavirus statistics causes emotional bias: a social media text mining perspective","linjiang guo, zijian feng, yuxue chi, mingzhu wang, yijun liu","computers and society","while covid-19 has impacted humans for a long time, people search the web for pandemic-related information, causing anxiety. from a theoretic perspective, previous studies have confirmed that the number of covid-19 cases can cause negative emotions, but how statistics of different dimensions, such as the number of imported cases, the number of local cases, and the number of government-designated lockdown zones, stimulate people's emotions requires detailed understanding. in order to obtain the views of people on covid-19, this paper first proposes a deep learning model which classifies texts related to the pandemic from text data with place labels. next, it conducts a sentiment analysis based on multi-task learning. finally, it carries out a fixed-effect panel regression with outputs of the sentiment analysis. the performance of the algorithm shows a promising result. the empirical study demonstrates while the number of local cases is positively associated with risk perception, the number of imported cases is negatively associated with confidence levels, which explains why citizens tend to ascribe the protracted pandemic to foreign factors. besides, this study finds that previous pandemic hits cities recover slowly from the suffering, while local governments' spending on healthcare can improve the situation. our study illustrates the reasons for risk perception and confidence based on different sources of statistical information due to cognitive bias. it complements the knowledge related to epidemic information. it also contributes to a framework that combines sentiment analysis using advanced deep learning technology with the empirical regression method.",2022-11-16
"auto-outlier fusion technique for chest x-ray classification with multi-head attention mechanism","yuru jing, zixuan li","image and video processing","a chest x-ray is one of the most widely available radiological examinations for diagnosing and detecting various lung illnesses. the national institutes of health (nih) provides an extensive database, chestx-ray8 and chestxray14, to help establish a deep learning community for analysing and predicting lung diseases. chestx-ray14 consists of 112,120 frontal-view x-ray images of 30,805 distinct patients with text-mined fourteen disease image labels, where each image has multiple labels and has been utilised in numerous research in the past. to our current knowledge, no previous study has investigated outliers and multi-label impact for a single x-ray image during the preprocessing stage. the effect of outliers is mitigated in this paper by our proposed auto-outlier fusion technique. the image label is regenerated by concentrating on a particular factor in one image. the final cleaned dataset will be used to compare the mechanisms of multi-head self-attention and multi-head attention with generalised max-pooling.",2022-11-15
"geoai for knowledge graph construction: identifying causality between cascading events to support environmental resilience research","yuanyuan tian, wenwen li","artificial intelligence","knowledge graph technology is considered a powerful and semantically enabled solution to link entities, allowing users to derive new knowledge by reasoning data according to various types of reasoning rules. however, in building such a knowledge graph, events modeling, such as that of disasters, is often limited to single, isolated events. the linkages among cascading events are often missing in existing knowledge graphs. this paper introduces our geoai (geospatial artificial intelligence) solutions to identify causality among events, in particular, disaster events, based on a set of spatially and temporally-enabled semantic rules. through a use case of causal disaster events modeling, we demonstrated how these defined rules, including theme-based identification of correlated events, spatiotemporal co-occurrence constraint, and text mining of event metadata, enable the automatic extraction of causal relationships between different events. our solution enriches the event knowledge base and allows for the exploration of linked cascading events in large knowledge graphs, therefore empowering knowledge query and discovery.",2022-11-11
"arabic text mining","sumaia mohammed al-ghuribi, shahrul azman mohd noah","information retrieval","the rapid growth of the internet has increased the number of online texts. this led to the rapid growth of the number of online texts in the arabic language. the enormous amount of text must be organized into classes to make the analysis process and text retrieval easier. text classification is, therefore, a key component of text mining. there are numerous systems and approaches for categorizing literature in english, european (french, german, spanish), and asian (chinese, japanese). in contrast, there are relatively few studies on categorizing arabic literature due to the difficulty of the arabic language. in this work, a brief explanation of key ideas relevant to arabic text mining are introduced then a new classification system for the arabic language is presented using light stemming and classifier na√Øve bayesian (cnb). texts from two classes: politics and sports, are included in our corpus. some texts are added to the system, and the system correctly classified them, demonstrating the effectiveness of the system.",2022-11-04
"automated code extraction from discussion board text dataset","sina mahdipour saravani, sadaf ghaffari, yanye luther, james folkestad, marcia moraes","machine learning","this study introduces and investigates the capabilities of three different text mining approaches, namely latent semantic analysis, latent dirichlet analysis, and clustering word vectors, for automating code extraction from a relatively small discussion board dataset. we compare the outputs of each algorithm with a previous dataset that was manually coded by two human raters. the results show that even with a relatively small dataset, automated approaches can be an asset to course instructors by extracting some of the discussion codes, which can be used in epistemic network analysis.",2022-10-31
"can current explainability help provide references in clinical notes to support humans annotate medical codes?","byung-hak kim, zhongfen deng, philip s. yu, varun ganapathi","machine learning","the medical codes prediction problem from clinical notes has received substantial interest in the nlp community, and several recent studies have shown the state-of-the-art (sota) code prediction results of full-fledged deep learning-based methods. however, most previous sota works based on deep learning are still in early stages in terms of providing textual references and explanations of the predicted codes, despite the fact that this level of explainability of the prediction outcomes is critical to gaining trust from professional medical coders. this raises the important question of how well current explainability methods apply to advanced neural network models such as transformers to predict correct codes and present references in clinical notes that support code prediction. first, we present an explainable read, attend, and code (xrac) framework and assess two approaches, attention score-based xrac-attn and model-agnostic knowledge-distillation-based xrac-kd, through simplified but thorough human-grounded evaluations with sota transformer-based model, rac. we find that the supporting evidence text highlighted by xrac-attn is of higher quality than xrac-kd whereas xrac-kd has potential advantages in production deployment scenarios. more importantly, we show for the first time that, given the current state of explainability methodologies, using the sota medical codes prediction system still requires the expertise and competencies of professional coders, even though its prediction accuracy is superior to that of human coders. this, we believe, is a very meaningful step toward developing explainable and accurate machine learning systems for fully autonomous medical code prediction from clinical notes.",2022-10-28
"machine and deep learning methods with manual and automatic labelling for news classification in bangla language","istiak ahmad, fahad alqurashi, rashid mehmood","artificial intelligence","research in natural language processing (nlp) has increasingly become important due to applications such as text classification, text mining, sentiment analysis, pos tagging, named entity recognition, textual entailment, and many others. this paper introduces several machine and deep learning methods with manual and automatic labelling for news classification in the bangla language. we implemented several machine (ml) and deep learning (dl) algorithms. the ml algorithms are logistic regression (lr), stochastic gradient descent (sgd), support vector machine (svm), random forest (rf), and k-nearest neighbour (knn), used with bag of words (bow), term frequency-inverse document frequency (tf-idf), and doc2vec embedding models. the dl algorithms are long short-term memory (lstm), bidirectional lstm (bilstm), gated recurrent unit (gru), and convolutional neural network (cnn), used with word2vec, glove, and fasttext word embedding models. we develop automatic labelling methods using latent dirichlet allocation (lda) and investigate the performance of single-label and multi-label article classification methods. to investigate performance, we developed from scratch potrika, the largest and the most extensive dataset for news classification in the bangla language, comprising 185.51 million words and 12.57 million sentences contained in 664,880 news articles in eight distinct categories, curated from six popular online news portals in bangladesh for the period 2014-2020. gru and fasttext with 91.83% achieve the highest accuracy for manually-labelled data. for the automatic labelling case, knn and doc2vec at 57.72% and 75% achieve the highest accuracy for single-label and multi-label data, respectively. the methods developed in this paper are expected to advance research in bangla and other languages.",2022-10-19
"using bottleneck adapters to identify cancer in clinical notes under low-resource constraints","omid rohanian, hannah jauncey, mohammadmahdi nouriborji, vinod kumar chauhan, bronner p. gon√ßalves, christiana kartsonaki, isaric clinical characterisation group, laura merson, david clifton","computation and language","processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical nlp. in this work, we evaluate a broad set of machine learning techniques ranging from simple rnns to specialised transformers such as biobert on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. furthermore, we specifically employ efficient fine-tuning methods from nlp, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. our evaluations suggest that fine-tuning a frozen bert model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised biobert model. based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. the code used in the experiments are going to be made available at this https url.",2022-10-17
"smtce: a social media text classification evaluation benchmark and bertology models for vietnamese","luan thanh nguyen, kiet van nguyen, ngan luu-thuy nguyen","computation and language","text classification is a typical natural language processing or computational linguistics task with various interesting applications. as the number of users on social media platforms increases, data acceleration promotes emerging studies on social media text classification (smtc) or social media text mining on these valuable resources. in contrast to english, vietnamese, one of the low-resource languages, is still not concentrated on and exploited thoroughly. inspired by the success of the glue, we introduce the social media text classification evaluation (smtce) benchmark, as a collection of datasets and models across a diverse set of smtc tasks. with the proposed benchmark, we implement and analyze the effectiveness of a variety of multilingual bert-based models (mbert, xlm-r, and distilmbert) and monolingual bert-based models (phobert, vibert, velectra, and vibert4news) for tasks in the smtce benchmark. monolingual models outperform multilingual models and achieve state-of-the-art results on all text classification tasks. it provides an objective assessment of multilingual and monolingual bert-based models on the benchmark, which will benefit future studies about bertology in the vietnamese language.",2022-09-21
"the language and social behavior of innovators","a. fronzetti colladon, l. toschi, e. ughetto, f. greco","computation and language","innovators are creative people who can conjure the ground-breaking ideas that represent the main engine of innovative organizations. past research has extensively investigated who innovators are and how they behave in work-related activities. in this paper, we suggest that it is necessary to analyze how innovators behave in other contexts, such as in informal communication spaces, where knowledge is shared without formal structure, rules, and work obligations. drawing on communication and network theory, we analyze about 38,000 posts available in the intranet forum of a large multinational company. from this, we explain how innovators differ from other employees in terms of social network behavior and language characteristics. through text mining, we find that innovators write more, use a more complex language, introduce new concepts/ideas, and use positive but factual-based language. understanding how innovators behave and communicate can support the decision-making processes of managers who want to foster innovation.",2022-09-20
"hapi: a large-scale longitudinal dataset of commercial ml api predictions","lingjiao chen, zhihua jin, sabri eyuboglu, christopher r√©, matei zaharia, james zou","software engineering","commercial ml apis offered by providers such as google, amazon and microsoft have dramatically simplified ml adoption in many applications. numerous companies and academics pay to use ml apis for tasks such as object detection, ocr and sentiment analysis. different ml apis tackling the same task can have very heterogeneous performance. moreover, the ml models underlying the apis also evolve over time. as ml apis rapidly become a valuable marketplace and a widespread way to consume machine learning, it is critical to systematically study and compare different apis with each other and to characterize how apis change over time. however, this topic is currently underexplored due to the lack of data. in this paper, we present hapi (history of apis), a longitudinal dataset of 1,761,417 instances of commercial ml api applications (involving apis from amazon, google, ibm, microsoft and other providers) across diverse tasks including image tagging, speech recognition and text mining from 2020 to 2022. each instance consists of a query input for an api (e.g., an image or text) along with the api's output prediction/annotation and confidence scores. hapi is the first large-scale dataset of ml api usages and is a unique resource for studying ml-as-a-service (mlaas). as examples of the types of analyses that hapi enables, we show that ml apis' performance change substantially over time--several apis' accuracies dropped on specific benchmark datasets. even when the api's aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. such changes can substantially impact the entire analytics pipelines that use some ml api as a component. we further use hapi to study commercial apis' performance disparities across demographic subgroups over time. hapi can stimulate more research in the growing field of mlaas.",2022-09-18
"chemnlp: a natural language processing based library for materials chemistry text data","kamal choudhary, mathew l. kelley","materials science","in this work, we present the chemnlp library that can be used for 1) curating open access datasets for materials and chemistry literature, developing and comparing traditional machine learning, transformers and graph neural network models for 2) classifying and clustering texts, 3) named entity recognition for large-scale text-mining, 4) abstractive summarization for generating titles of articles from abstracts, 5) text generation for suggesting abstracts from titles, 6) integration with density functional theory dataset for identifying potential candidate materials such as superconductors, and 7) web-interface development for text and reference query. we primarily use the publicly available arxiv and pubchem datasets but the tools can be used for other datasets as well. moreover, as new models are developed, they can be easily integrated in the library. chemnlp is available at the websites: this https url and this https url.",2022-09-17
"unicausal: unified benchmark and repository for causal text mining","fiona anting tan, xinyu zuo, see-kiong ng","computation and language","current causal text mining datasets vary in objectives, data coverage, and annotation schemes. these inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. to address these issues, we propose unicausal, a unified benchmark for causal text mining across three tasks: (i) causal sequence classification, (ii) cause-effect span detection and (iii) causal pair classification. we consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. to create an initial benchmark, we fine-tuned bert pre-trained language models to each task, achieving 70.10% binary f1, 52.42% macro f1, and 84.68% binary f1 scores respectively.",2022-08-19
"twitter attribute classification with q-learning on bitcoin price prediction","sattarov otabek, jaeyoung choi","social and information networks","aspiring to achieve an accurate bitcoin price prediction based on people's opinions on twitter usually requires millions of tweets, using different text mining techniques (preprocessing, tokenization, stemming, stop word removal), and developing a machine learning model to perform the prediction. these attempts lead to the employment of a significant amount of computer power, central processing unit (cpu) utilization, random-access memory (ram) usage, and time. to address this issue, in this paper, we consider a classification of tweet attributes that effects on price changes and computer resource usage levels while obtaining an accurate price prediction. to classify tweet attributes having a high effect on price movement, we collect all bitcoin-related tweets posted in a certain period and divide them into four categories based on the following tweet attributes: $(i)$ the number of followers of the tweet poster, $(ii)$ the number of comments on the tweet, $(iii)$ the number of likes, and $(iv)$ the number of retweets. we separately train and test by using the q-learning model with the above four categorized sets of tweets and find the best accurate prediction among them. especially, we design several reward functions to improve the prediction accuracy of the q-leaning. we compare our approach with a classic approach where all bitcoin-related tweets are used as input data for the model, by analyzing the cpu workloads, ram usage, memory, time, and prediction accuracy. the results show that tweets posted by users with the most followers have the most influence on a future price, and their utilization leads to spending 80\% less time, 88.8\% less cpu consumption, and 12.5\% more accurate predictions compared with the classic approach.",2022-08-04
"evaluating and improving social awareness of energy communities through semantic network analysis of online news","c. piselli, a. fronzetti colladon, l. segneri, a. l. pisello","social and information networks","the implementation of energy communities represents a cross-disciplinary phenomenon that has the potential to support the energy transition while fostering citizens' participation throughout the energy system and their exploitation of renewables. an important role is played by online information sources in engaging people in this process and increasing their awareness of associated benefits. in this view, this work analyses online news data on energy communities to understand people's awareness and the media importance of this topic. we use the semantic brand score (sbs) indicator as an innovative measure of semantic importance, combining social network analysis and text mining methods. results show different importance trends for energy communities and other energy and society-related topics, also allowing the identification of their connections. our approach gives evidence to information gaps and possible actions that could be taken to promote a low-carbon energy transition.",2022-08-03
"joint learning-based causal relation extraction from biomedical literature","dongling li, pengchao wu, yuehu dong, jinghang gu, longhua qian, guodong zhou","computation and language","causal relation extraction of biomedical entities is one of the most complex tasks in biomedical text mining, which involves two kinds of information: entity relations and entity functions. one feasible approach is to take relation extraction and function detection as two independent sub-tasks. however, this separate learning method ignores the intrinsic correlation between them and leads to unsatisfactory performance. in this paper, we propose a joint learning model, which combines entity relation extraction and entity function detection to exploit their commonality and capture their inter-relationship, so as to improve the performance of biomedical causal relation extraction. meanwhile, during the model training stage, different function types in the loss function are assigned different weights. specifically, the penalty coefficient for negative function instances increases to effectively improve the precision of function detection. experimental results on the biocreative-v track 4 corpus show that our joint learning model outperforms the separate models in bel statement extraction, achieving the f1 scores of 58.4% and 37.3% on the test set in stage 2 and stage 1 evaluations, respectively. this demonstrates that our joint learning system reaches the state-of-the-art performance in stage 2 compared with other systems.",2022-08-02
"knowledge-driven mechanistic enrichment of the preeclampsia ignorome","tiffany j. callahan, adrianne l. stefanski, jin-dong kim, william a. baumgartner jr., jordan m. wyrwa, lawrence e. hunter","genomics","preeclampsia is a leading cause of maternal and fetal morbidity and mortality. currently, the only definitive treatment of preeclampsia is delivery of the placenta, which is central to the pathogenesis of the disease. transcriptional profiling of human placenta from pregnancies complicated by preeclampsia has been extensively performed to identify differentially expressed genes (degs). the decisions to investigate degs experimentally are biased by many factors, causing many degs to remain uninvestigated. a set of degs which are associated with a disease experimentally, but which have no known association to the disease in the literature are known as the ignorome. preeclampsia has an extensive body of scientific literature, a large pool of deg data, and only one definitive treatment. tools facilitating knowledge-based analyses, which are capable of combining disparate data from many sources in order to suggest underlying mechanisms of action, may be a valuable resource to support discovery and improve our understanding of this disease. in this work we demonstrate how a biomedical knowledge graph (kg) can be used to identify novel preeclampsia molecular mechanisms. existing open source biomedical resources and publicly available high-throughput transcriptional profiling data were used to identify and annotate the function of currently uninvestigated preeclampsia-associated degs. experimentally investigated genes associated with preeclampsia were identified from pubmed abstracts using text-mining methodologies. the relative complement of the text-mined- and meta-analysis-derived lists were identified as the uninvestigated preeclampsia-associated degs (n=445), i.e., the preeclampsia ignorome. using the kg to investigate relevant degs revealed 53 novel clinically relevant and biologically actionable mechanistic associations.",2022-07-28
"patent search using triplet networks based fine-tuned scibert","utku umur acikalin, mucahid kutlu","information retrieval","in this paper, we propose a novel method for the prior-art search task. we fine-tune scibert transformer model using triplet network approach, allowing us to represent each patent with a fixed-size vector. this also enables us to conduct efficient vector similarity computations to rank patents in query time. in our experiments, we show that our proposed method outperforms baseline methods.",2022-07-23
"learning from what we know: how to perform vulnerability prediction using noisy historical data","aayush garg, renzo degiovanni, matthieu jimenez, maxime cordy, mike papadakis, yves letraon","software engineering","vulnerability prediction refers to the problem of identifying system components that are most likely to be vulnerable. typically, this problem is tackled by training binary classifiers on historical data. unfortunately, recent research has shown that such approaches underperform due to the following two reasons: a) the imbalanced nature of the problem, and b) the inherently noisy historical data, i.e., most vulnerabilities are discovered much later than they are introduced. this misleads classifiers as they learn to recognize actual vulnerable components as non-vulnerable. to tackle these issues, we propose trovon, a technique that learns from known vulnerable components rather than from vulnerable and non-vulnerable components, as typically performed. we perform this by contrasting the known vulnerable, and their respective fixed components. this way, trovon manages to learn from the things we know, i.e., vulnerabilities, hence reducing the effects of noisy and unbalanced data. we evaluate trovon by comparing it with existing techniques on three security-critical open source systems, i.e., linux kernel, openssl, and wireshark, with historical vulnerabilities that have been reported in the national vulnerability database (nvd). our evaluation demonstrates that the prediction capability of trovon significantly outperforms existing vulnerability prediction techniques such as software metrics, imports, function calls, text mining, devign, lstm, and lstm-rf with an improvement of 40.84% in matthews correlation coefficient (mcc) score under clean training data settings, and an improvement of 35.52% under realistic training data settings.",2022-07-22
"research trends and applications of data augmentation algorithms","joao fonseca, fernando bacao","machine learning","in the machine learning research community, there is a consensus regarding the relationship between model complexity and the required amount of data and computation power. in real world applications, these computational requirements are not always available, motivating research on regularization methods. in addition, current and past research have shown that simpler classification algorithms can reach state-of-the-art performance on computer vision tasks given a robust method to artificially augment the training dataset. because of this, data augmentation techniques became a popular research topic in recent years. however, existing data augmentation methods are generally less transferable than other regularization methods. in this paper we identify the main areas of application of data augmentation algorithms, the types of algorithms used, significant research trends, their progression over time and research gaps in data augmentation literature. to do this, the related literature was collected through the scopus database. its analysis was done following network science, text mining and exploratory analysis approaches. we expect readers to understand the potential of data augmentation, as well as identify future research directions and open questions within data augmentation research.",2022-07-18
"keyword extraction in scientific documents","susie xi rao, piriyakorn piriyatamwong, parijat ghoshal, sara nasirian, emmanuel de salis, sandra mitroviƒá, michael wechner, vanya brucker, peter egger, ce zhang","computation and language","the scientific publication output grows exponentially. therefore, it is increasingly challenging to keep track of trends and changes. understanding scientific documents is an important step in downstream tasks such as knowledge graph building, text mining, and discipline classification. in this workshop, we provide a better understanding of keyword and keyphrase extraction from the abstract of scientific publications.",2022-07-05
"mining tourism experience on twitter: a case study","davide stirparo, beatrice penna, mohammad kazemi, ariona shashaj","social and information networks","with the increase of digital data and social network platforms the impact of social media science in driving company decision related to product/service features and customer care operations is becoming more crucial. in particular, platform such as twitter where people can share experience about almost everything can drastically impact the reputation and offering of a company as well as of a place or tourism site. text mining tools are researched and proposed in literature in order to gain value and perform trend topics and sentiment analysis on twitter. as data are the fuels for these models, the ""right"" ones, i.e the domain-related ones makes a difference on their accuracy. in this paper, we describe a pipeline of \textit{dataops / mlops} operations performed over a tourism related twitter dataset in order to comprehend tourism motivation and interest. the gained knowledge can be exploit, by the travel/hospitality industry in order to develop data-driven strategic service, and by travelers which can consume relevant information about tourist destination.",2022-06-30
